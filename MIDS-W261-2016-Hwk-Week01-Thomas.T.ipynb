{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale\n",
    "##Section 3\n",
    "##Homework, Week 1\n",
    "##Name: T.Thomas\n",
    "##Email: tgthomas@berkeley.edu\n",
    "##Submission Date: 1/22/2016 1:30AM PST\n",
    "\n",
    "**nbviewer link:**\n",
    "http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/leu326mah0c148f/MIDS-W261-2016-Hwk-Week01-Thomas.T.ipynb?flush_cache=true\n",
    "\n",
    "**pdf link:**\n",
    "https://www.dropbox.com/s/ur1ehwhqi27s4da/MIDS-W261-2016-Hwk-Week01-Thomas.T.pdf?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Big Data is a general term used to represent very large and/or complex data sets that cannot be consumed for analysis or processing using traditional sequential data processing applications. This applies to limitations in Storage, Processing and Throughput.\n",
    "\n",
    "The above definition however does not tell us anything about how big really is Big Data and that size component has evolved over time with technology. 10 Years ago 200GB could have been big. Today 2 Terabytes or some data that does not fit on a typical hardrive might be considered big. Processing this data using a traditional application using a single desktop/laptop might be impractical in terms of Storage, Processing time and Throughput.\n",
    "\n",
    "In the Banking and Finance domain where I have worked over the past 10 years, a typical 'Big Data' data set could be the millions of daily transcations executed on each of the banks millions of customers. These transcation data need to be analyzed for different reasons like fraud detection, new cross selling opportunities and customer retention intiatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea here would be to fit the test data *** $T$ *** using the different polynomial estimators *** $gN(x)$ *** having degree model order *** $N = [1,2,3,4,5]$* ** Within each estimator fit test, we measure the variance, bias and irreducible error for that estimator model. Then finally we choose the best model where both Bias and Variance are minimized.\n",
    "\n",
    "However to get a good estimate of our model, we will need as many potential data sets as possible, but we have only one sample data set to work with * $T$ *. To simulate potential data sets, we will bootstrap data sets by randomly drawing samples from T up to say 50 times, to generate 50 different sample data sets. We will repeat the fitting process for each of these 50 data sets to measure variance and bias.\n",
    "\n",
    "To measure ***variance*** for each model *$g_N(x)$*, we estimate an average estimator over all the 50 datasets for that model,  and consider variance to be a measure of how much each single estimate from each individual data set deviates from the average estimator from all data sets. The variance is essentially measuring how each of the estimators vary with each other – if there is too much variation, then model has high variance. Formally:\n",
    "\n",
    "***variance*** = the average squared difference between any single data-set-dependent estimate $g_N(x)$ and the average value of estimated $E[g_N(x)]$ over all datasets.\n",
    "\n",
    "***variance = $E[ ( g_N(x) – E[g_N(x)])^2 ]$ ***\n",
    "\n",
    "To measure bias for each model, we estimate and average estimator fit over all the data sets and consider bias to be a measure of how much this average estimator deviates from the true model representing T. Formally:\n",
    "\n",
    "***bias*** = how much the average estimator fit over datasets E[gN(x)]deviates from the value of the underlying target function f(x).\n",
    "\n",
    "***bias = $E[g_N(x)]- f(x)$ ***\n",
    "\n",
    "In addition to *bias* and *variance* we also determine each models goodness of fit. For a new data point\n",
    "\n",
    "$x^*, y^* = g_N^*(x) + \\epsilon$ \n",
    "\n",
    "we consider ***Expected Prediction Error***, representing the squared difference between the model's prediction $g_N^{*}(x)$ of the observarion $y^{*}$. The expected prediction error is then:\n",
    "\n",
    "$Err(x)=E[(g_N^{*}(x) − y^{*})^2]$\n",
    "\n",
    "This error may then be decomposed into bias and variance components:\n",
    "\n",
    "$Err(x)=Bias^2 + Variance + Irreducible Error$\n",
    "\n",
    "That third term, irreducible error, is the noise term, that cannot be modeled away. It cannot be reduced by any model. \n",
    "\n",
    "\n",
    "<img src=\"https://theclevermachine.files.wordpress.com/2013/04/bias-variance-tradeoff.png\" height=\"300\" width=\"300\" align=left hspace=100 vspace=10 border=100>\n",
    "\n",
    "\n",
    "\n",
    "   Pseudo code describing the model estimation process\n",
    "\n",
    "    T = Test Data with k observations\n",
    "    S = subset of n observations randomly sampled from T \n",
    "\n",
    "    For N = 1 to 5 #choose polynomial with degree\n",
    "        For i = 1 to 50 #choose 50 random samples of n observations from T\n",
    "\n",
    "             S = random.sample(T, n)         \n",
    "             Gx[i] = Estimate gN(x) for S \n",
    "         \n",
    "        avg_est = AVG(G(x))\n",
    "        variance[N] = SUM_i= 1 to 50 [(Gx[i] – avg_est)^2]\n",
    "        bias[N] = SUMi= 1 to 50 [(avg_est – f(x))^2]\n",
    "\n",
    "\n",
    "Choose Model where bias and variance are smallest. If there are multiple models with low variance and bias, then pick the simpler model where possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.1. Read through the provided control script (pNaiveBayes.sh)    and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "***NOTE:***\n",
    "\n",
    "* The the same mapper.py and reducer.py are used for all questions 1.2 - 1.5\n",
    "* I tested 1.3 without smoothing, but since I am using the same code for the remaining 1.4-1.5, the outputs for 1.3-1.5 all include smoothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Tigi Thomas\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#fixed list of stop words\n",
    "stopwords = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "             'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','why','will','with','would','yet','you','your']\n",
    "\n",
    "#Use some pre-compiled re-gex for punctuation and numbers\n",
    "punctpattern = re.compile(r'[\\.\\^\\$\\*\\+\\-\\=\\{\\}\\[\\]\\\\\\|\\(\\)<>-@#%_=!:;,/\\'\\\"]')\n",
    "numpatt1 = re.compile(r'\\b[0-9]{2,100}\\b')\n",
    "numpatt2 = re.compile(r'[0-9]{2,100}\\b')\n",
    "numpatt3 = re.compile(r'\\b[0-9]{2,100}')\n",
    "\n",
    "# Preprocess any text\n",
    "def preprocess_txt(s): \n",
    "    s = s.lower()\n",
    "    s = re.sub(punctpattern, r'',s)\n",
    "    s = re.sub(numpatt1, r'', s )\n",
    "    s = re.sub(numpatt2, r'', s )\n",
    "    s = re.sub(numpatt3, r'', s )\n",
    "    return s\n",
    "\n",
    "# -- check ---\n",
    "#print sys.argv\n",
    "\n",
    "#initialize arguments\n",
    "count = 0\n",
    "filename = sys.argv[1]\n",
    "findwords = []\n",
    "\n",
    "#set up word list if provied or check * for non\n",
    "if (len(sys.argv) >= 2 and sys.argv[2] != \"*\"):\n",
    "    findwords = set(re.split(\" \",sys.argv[2].lower()))\n",
    "\n",
    "with open (filename, \"rU\") as myfile:\n",
    "#Please insert your code\n",
    "    for line in myfile:\n",
    "        line = line.lower()\n",
    "        emaildata = line.split(\"\\t\")\n",
    "        emaildatalen = len(emaildata)\n",
    "        \n",
    "        #don't really care about the first item, \n",
    "        #start from spam or not.. \n",
    "        if(emaildatalen >= 2):            \n",
    "            if(emaildatalen >= 3 ):\n",
    "                subject = emaildata[2]\n",
    "            else:\n",
    "                subject = \" \"\n",
    "            \n",
    "            if(emaildatalen == 4 ):\n",
    "                body = emaildata[3]\n",
    "            else:\n",
    "                body = \" \"\n",
    "            \n",
    "            #Get subject and body of email together\n",
    "            emailcontent = subject + \" \"  + body \n",
    "            \n",
    "            #pre-process to remove punctuation etc.\n",
    "            emailcontent = preprocess_txt(emailcontent)\n",
    "            \n",
    "            #extract words now and filter out stop-words\n",
    "            words = emailcontent.split()\n",
    "            filtered_words = [tk for tk in words if tk not in stopwords]\n",
    "\n",
    "            #we write the ID , current Spam classification, total word count in document\n",
    "            #the output file will contain one row for every document\n",
    "            if emaildata[1] == \"1\":\n",
    "                outtxt = emaildata[0]+'\\t'+ \"1\" +'\\t'+ str(len(filtered_words)) \n",
    "            else:\n",
    "                outtxt = emaildata[0]+'\\t'+ \"0\" +'\\t'+ str(len(filtered_words)) \n",
    "    \n",
    "            #Prepare our word count list\n",
    "            #Add word to dict if in findword list or all words '*' specified\n",
    "            wc = {}\n",
    "            for word in filtered_words:\n",
    "                if( (len(findwords) == 0) or (word in findwords)):\n",
    "                    wc[word] = wc.get(word, 0) + 1\n",
    "            \n",
    "            #Stick the occurence of word=count for every word in document\n",
    "            #for faster processing on the reducer side.\n",
    "            wordcounttxt = \"\\t*\"\n",
    "            if(len(wc) > 0):\n",
    "                wordcounttxt = \"\";\n",
    "                for word, count in wc.iteritems():\n",
    "                    wordcounttxt += '\\t'+word+\"=\"+str(count)\n",
    "            \n",
    "            #write out document row with \n",
    "            print outtxt + wordcounttxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Reducer Code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: T.Thomas\n",
    "## Description: reducer code for HW1.2-1.5\n",
    "import sys\n",
    "import math\n",
    "\n",
    "\n",
    "def printcollection(Title, coll):\n",
    "    if(Title != \"\"):\n",
    "        print \"\"\n",
    "        print Title          \n",
    "    for k, v in coll.iteritems():\n",
    "        print k, v\n",
    "\n",
    "\n",
    "# some dict structures to keep track of intermediate data\n",
    "wc = {}\n",
    "spwc = {}\n",
    "nb = {}\n",
    "w_condprob = {}\n",
    "testdata = {}\n",
    "\n",
    "#Going to add smoothing params if specified\n",
    "lp_smooth = False\n",
    "if (sys.argv[1] == \"1\"):\n",
    "    lp_smooth = True\n",
    "\n",
    "# -- check ---\n",
    "#print sys.argv\n",
    "\n",
    "# we skip the very first argument and the rest are the file \n",
    "# names that are being passed in\n",
    "for filename in sys.argv[2:]:\n",
    "    #open each file and get the counts into Wc\n",
    "    with open (filename, \"rU\") as myfile:\n",
    "        for line in myfile:\n",
    "            item = line.split(\"\\t\")\n",
    "            \n",
    "            #extract total spam=1 and spam=0 doc/email counts\n",
    "            nb[\"spam\"+item[1]+\"dc\"] = nb.get(\"spam\"+item[1]+\"dc\", 0) + 1\n",
    "            #extract spam=1 and spam=0 word counts\n",
    "            nb[\"spam\"+item[1]+\"wc\"] = nb.get(\"spam\"+item[1]+\"wc\", 0) + eval(item[2])\n",
    "\n",
    "            #extract the words and their counts per document\n",
    "            #this way we have how many times word occurs in document\n",
    "            #and how many times it occurs in a spam document\n",
    "            if(item[3].strip() != \"*\"):\n",
    "                for itm in item[3:]:\n",
    "                    word, count = itm.strip().split(\"=\",1)\n",
    "                    wc[word] = wc.get(word, 0) + eval(count)\n",
    "                    if(item[1] == \"1\"):                  \n",
    "                        spwc[word] = spwc.get(word, 0) + eval(count)\n",
    "                    #nb[item[0]+\"_InSpamCount\"] = nb.get(item[0]+\"_InSpamCount\", 0) + eval(item[2])\n",
    "            testdata[item[0]] = item[1:]\n",
    "\n",
    "#Calculate prior probabilities - these will be used everywhere.            \n",
    "prior_spam =  ( nb[\"spam1dc\"] / float( nb[\"spam1dc\"] + nb[\"spam0dc\"] ) ) \n",
    "prior_notspam = 1 - prior_spam \n",
    "\n",
    "#setup smoothing parameters if smoothing was specified. see line #58-59 in pNaiveBayes.sh\n",
    "lp_num = 0\n",
    "lp_denom = 0   \n",
    "if (lp_smooth):\n",
    "    lp_num = 1\n",
    "    lp_denom = len(wc) \n",
    "    \n",
    "#Here we pre-compute the conditional probablitly for each word\n",
    "#P(word in email | spam) &  #P(word in email | not spam)\n",
    "for word, count in wc.iteritems():\n",
    "    in_spam_count = spwc.get(word,0)\n",
    "    not_in_spam_count = (count - in_spam_count)\n",
    "    spam_wordcount = nb.get(\"spam1wc\")\n",
    "    not_spam_wordcount = nb.get(\"spam0wc\")\n",
    "    \n",
    "    spam_condprob =  (in_spam_count + lp_num)/ float(spam_wordcount + lp_num)\n",
    "    notspam_condprob = (not_in_spam_count + lp_num) / float(not_spam_wordcount + lp_num)\n",
    "    \n",
    "    # ----------- Debug check if we have any prob that are 0.0 -----------------\n",
    "    #if(spam_condprob == 0.0 or notspam_condprob == 0.0):\n",
    "    #   print word, count, in_spam_count,  not_in_spam_count   \n",
    "    \n",
    "    w_condprob[word] = [spam_condprob, notspam_condprob]\n",
    "          \n",
    "#print all words and the count\n",
    "#printcollection(\"-----------Total Word Count-----------------\", wc)\n",
    "\n",
    "#print in-spam count for each word\n",
    "#printcollection(\"-----------In Spam Word Count-----------------\", spwc)\n",
    "\n",
    "#print some NB model parameters\n",
    "printcollection(\"-----------Counts for Naive Bayes Model -------------\", nb)\n",
    "\n",
    "#print computed probabilities after running thru training set\n",
    "print \"\"\n",
    "print \"------Computed Probabilities from Training Set---\"\n",
    "print \"P(prior_spam) = {0:.5f}\".format(prior_spam)\n",
    "print \"P(prior_not_Spam) = {0:.5f}\".format(prior_notspam)\n",
    "\n",
    "print \"\"\n",
    "#print \"Word [ P(spam | word in email), P(not spam | word in email) ]\"\n",
    "#print \"-------------------------------------------------------------\"\n",
    "#printcollection(\"\", w_condprob)\n",
    "\n",
    "\n",
    "#print classification - classify test data set\n",
    "print \"\"\n",
    "print \"RESULTS: Classification of Test Data ----------\"\n",
    "print \"\"\n",
    "print \"ID  Truth(Spam/Ham : 1/0) Class(Spam/Ham : 1/0)\"\n",
    "print \"-----------------------------------------------\"\n",
    "result = {}\n",
    "\n",
    "#run the model on our data set classifying all emails.\n",
    "#note: \n",
    "#the first item in w_condprob, namely w_condprob[word][0] stores P(word in email | spam)\n",
    "#the second item in w_condprob, namely w_condprob[word][1] stores P(word in email | not spam)\n",
    "doc_count = len(testdata)\n",
    "error_count = 0\n",
    "for docid, data in testdata.iteritems():\n",
    "    #data[0] is current classification\n",
    "    #data[1] is total word count in the document\n",
    "    #data[2:] contains all the individual words and their counts\n",
    "    #         and for each word we compute P(spam | word in email ) and P(not spam | word in email)\n",
    "    #         \n",
    "    p_spam = math.log(prior_spam)\n",
    "    p_notspam = math.log(prior_notspam)\n",
    "    if(data[2].strip() != \"*\"):        \n",
    "        for itm in data[2:]:            \n",
    "            word, count = itm.strip().split(\"=\",1)\n",
    "            p_spam += math.log(w_condprob[word][0])*eval(count)\n",
    "            p_notspam += math.log(w_condprob[word][1])*eval(count)\n",
    "        \n",
    "    if(p_spam > p_notspam): \n",
    "        print docid, data[0], \"1\"\n",
    "        if(data[0] != \"1\"):\n",
    "            error_count += 1\n",
    "    else:\n",
    "        print docid, data[0], \"0\"\n",
    "\n",
    "print \"\"\n",
    "print \"Model Error Rate = {0:.5f}\".format(error_count/doc_count)\n",
    "\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write pNaiveBayes.sh to file\n",
    "\n",
    "***NOTES/ASSUMPTIONS:*** \n",
    "* The \"enronemail_1h.txt\" file used hear was re-saved to a new file to fix line endings wich wc -l could not recognize.\n",
    "* I am using Smoothing for all examples, mainly to handle the Log(0) issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "## bc in the unix bench calculater.. you pipe and expression to it and \n",
    "## prints an output.\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "## do the split by the lines data.\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "##### ********* NOTE THE EXTRA 1 as first parameter to the reducer.. I am enforcing Smoothing for log(0) error\n",
    "./reducer.py 1 $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "***NOTE:***\n",
    "The same mapper and reducer are used for 1.2 - 1.5 but some code will be commented out to reduce outputs based on the HW question\n",
    "\n",
    "USAGE:\n",
    "pNaiveBayesWorking parallelmappers \"some word list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output for \"assistance\" word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "-----------Total Word Count-----------------\r\n",
      "assistance 10\r\n",
      "\r\n",
      "-----------In Spam Word Count-----------------\r\n",
      "assistance 8\r\n",
      "\r\n",
      "-----------Counts for Naive Bayes Model -------------\r\n",
      "spam0wc 8054\r\n",
      "spam0dc 56\r\n",
      "spam1dc 44\r\n",
      "spam1wc 10666\r\n",
      "\r\n",
      "------Computed Probabilities from Training Set---\r\n",
      "P(prior_spam) = 0.44000\r\n",
      "P(prior_not_Spam) = 0.56000\r\n",
      "\r\n",
      "Word [ P(spam | word in email), P(not spam | word in email) ]\r\n",
      "-------------------------------------------------------------\r\n",
      "assistance [0.0008437236336364489, 0.00037243947858472997]\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. \n",
    "\n",
    "To do so, make sure that\n",
    "  \n",
    "   * mapper.py and\n",
    "   * reducer.py \n",
    "\n",
    "that performs a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   * the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output word counts and Classification for \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "-----------Total Word Count-----------------\r\n",
      "assistance 10\r\n",
      "\r\n",
      "-----------In Spam Word Count-----------------\r\n",
      "assistance 8\r\n",
      "\r\n",
      "-----------Counts for Naive Bayes Model -------------\r\n",
      "spam0wc 8054\r\n",
      "spam0dc 56\r\n",
      "spam1dc 44\r\n",
      "spam1wc 10666\r\n",
      "\r\n",
      "------Computed Probabilities from Training Set---\r\n",
      "P(prior_spam) = 0.44000\r\n",
      "P(prior_not_Spam) = 0.56000\r\n",
      "\r\n",
      "Word [ P(spam | word in email), P(not spam | word in email) ]\r\n",
      "-------------------------------------------------------------\r\n",
      "assistance [0.0008437236336364489, 0.00037243947858472997]\r\n",
      "\r\n",
      "--------------- Classify Test Data ----------------\r\n",
      "\r\n",
      "ID  Truth(Spam/Ham : 1/0) Class(Spam/Ham : 1/0)\r\n",
      "-----------------------------------------------\r\n",
      "0001.2000-01-17.beck 0 0\r\n",
      "0018.1999-12-14.kaminski 0 0\r\n",
      "0004.2001-06-12.sa_and_hp 1 0\r\n",
      "0016.2003-12-19.gp 1 0\r\n",
      "0015.2001-07-05.sa_and_hp 1 0\r\n",
      "0009.1999-12-14.farmer 0 0\r\n",
      "0015.2001-02-12.kitchen 0 0\r\n",
      "0017.1999-12-14.kaminski 0 0\r\n",
      "0006.1999-12-13.kaminski 0 0\r\n",
      "0003.2000-01-17.beck 0 0\r\n",
      "0007.2004-08-01.bg 1 0\r\n",
      "0008.2001-06-12.sa_and_hp 1 0\r\n",
      "0007.2001-02-09.kitchen 0 0\r\n",
      "0008.2004-08-01.bg 1 0\r\n",
      "0010.1999-12-14.kaminski 0 0\r\n",
      "0015.2000-06-09.lokay 0 0\r\n",
      "0017.2003-12-18.gp 1 0\r\n",
      "0016.1999-12-15.farmer 0 0\r\n",
      "0012.2003-12-19.gp 1 0\r\n",
      "0012.2001-02-09.kitchen 0 0\r\n",
      "0005.1999-12-12.kaminski 0 1\r\n",
      "0007.2003-12-18.gp 1 0\r\n",
      "0014.2004-08-01.bg 1 0\r\n",
      "0006.2003-12-18.gp 1 0\r\n",
      "0011.1999-12-14.farmer 0 0\r\n",
      "0009.2001-02-09.kitchen 0 0\r\n",
      "0006.2001-02-08.kitchen 0 0\r\n",
      "0014.2001-07-04.sa_and_hp 1 0\r\n",
      "0010.1999-12-14.farmer 0 0\r\n",
      "0003.2004-08-01.bg 1 0\r\n",
      "0017.2004-08-01.bg 1 0\r\n",
      "0014.1999-12-14.kaminski 0 0\r\n",
      "0009.2001-06-26.sa_and_hp 1 0\r\n",
      "0013.2001-06-30.sa_and_hp 1 0\r\n",
      "0005.1999-12-14.farmer 0 0\r\n",
      "0010.2003-12-18.gp 1 0\r\n",
      "0013.1999-12-14.kaminski 0 0\r\n",
      "0001.2001-02-07.kitchen 0 0\r\n",
      "0008.2001-02-09.kitchen 0 0\r\n",
      "0016.2001-07-06.sa_and_hp 1 0\r\n",
      "0009.2003-12-18.gp 1 0\r\n",
      "0005.2000-06-06.lokay 0 0\r\n",
      "0001.2001-04-02.williams 0 0\r\n",
      "0011.2001-06-28.sa_and_hp 1 1\r\n",
      "0013.2004-08-01.bg 1 1\r\n",
      "0010.2004-08-01.bg 1 0\r\n",
      "0012.2000-06-08.lokay 0 0\r\n",
      "0006.2001-06-25.sa_and_hp 1 0\r\n",
      "0014.1999-12-15.farmer 0 0\r\n",
      "0009.2000-06-07.lokay 0 0\r\n",
      "0001.1999-12-10.farmer 0 0\r\n",
      "0008.2001-06-25.sa_and_hp 1 0\r\n",
      "0017.2001-04-03.williams 0 0\r\n",
      "0015.2003-12-19.gp 1 0\r\n",
      "0014.2001-02-12.kitchen 0 0\r\n",
      "0006.2004-08-01.bg 1 0\r\n",
      "0015.1999-12-15.farmer 0 0\r\n",
      "0016.2001-07-05.sa_and_hp 1 0\r\n",
      "0009.1999-12-13.kaminski 0 0\r\n",
      "0001.2000-06-06.lokay 0 0\r\n",
      "0002.2004-08-01.bg 1 1\r\n",
      "0002.1999-12-13.farmer 0 0\r\n",
      "0011.2003-12-18.gp 1 0\r\n",
      "0010.2001-06-28.sa_and_hp 1 1\r\n",
      "0004.1999-12-14.farmer 0 0\r\n",
      "0017.2004-08-02.bg 1 0\r\n",
      "0005.2001-06-23.sa_and_hp 1 0\r\n",
      "0007.1999-12-14.farmer 0 0\r\n",
      "0011.2001-06-29.sa_and_hp 1 0\r\n",
      "0008.2003-12-18.gp 1 0\r\n",
      "0007.2000-01-17.beck 0 0\r\n",
      "0003.1999-12-14.farmer 0 0\r\n",
      "0004.2004-08-01.bg 1 0\r\n",
      "0018.2003-12-18.gp 1 1\r\n",
      "0003.1999-12-10.kaminski 0 0\r\n",
      "0014.2003-12-19.gp 1 0\r\n",
      "0012.1999-12-14.farmer 0 0\r\n",
      "0003.2003-12-18.gp 1 0\r\n",
      "0004.1999-12-10.kaminski 0 1\r\n",
      "0018.2001-07-13.sa_and_hp 1 1\r\n",
      "0002.2001-02-07.kitchen 0 0\r\n",
      "0012.1999-12-14.kaminski 0 0\r\n",
      "0005.2003-12-18.gp 1 0\r\n",
      "0012.2000-01-17.beck 0 0\r\n",
      "0016.2004-08-01.bg 1 0\r\n",
      "0011.2004-08-01.bg 1 0\r\n",
      "0007.1999-12-13.kaminski 0 0\r\n",
      "0017.2000-01-17.beck 0 0\r\n",
      "0003.2001-02-08.kitchen 0 0\r\n",
      "0006.2001-04-03.williams 0 0\r\n",
      "0005.2001-02-08.kitchen 0 0\r\n",
      "0013.2001-04-03.williams 0 0\r\n",
      "0004.2001-04-02.williams 0 0\r\n",
      "0010.2001-02-09.kitchen 0 0\r\n",
      "0001.1999-12-10.kaminski 0 0\r\n",
      "0013.1999-12-14.farmer 0 0\r\n",
      "0015.1999-12-14.kaminski 0 0\r\n",
      "0016.2001-02-12.kitchen 0 0\r\n",
      "0002.2001-05-25.sa_and_hp 1 0\r\n",
      "0002.2003-12-18.gp 1 0\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "\n",
    "To do so, make sure that\n",
    "\n",
    "   * mapper.py counts all occurrences of a list of words, and\n",
    "   * reducer.py \n",
    "\n",
    "performs the multiple-word multinomial Naive Bayes classification via the chosen list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output word counts and Classification for \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "-----------Total Word Count-----------------\r\n",
      "assistance 10\r\n",
      "valium 3\r\n",
      "\r\n",
      "-----------In Spam Word Count-----------------\r\n",
      "assistance 8\r\n",
      "valium 3\r\n",
      "\r\n",
      "-----------Counts for Naive Bayes Model -------------\r\n",
      "spam0wc 8054\r\n",
      "spam0dc 56\r\n",
      "spam1dc 44\r\n",
      "spam1wc 10666\r\n",
      "\r\n",
      "------Computed Probabilities from Training Set---\r\n",
      "P(prior_spam) = 0.44000\r\n",
      "P(prior_not_Spam) = 0.56000\r\n",
      "\r\n",
      "Word [ P(spam | word in email), P(not spam | word in email) ]\r\n",
      "-------------------------------------------------------------\r\n",
      "assistance [0.0008437236336364489, 0.00037243947858472997]\r\n",
      "valium [0.00037498828161619947, 0.00012414649286157667]\r\n",
      "\r\n",
      "--------------- Classify Test Data ----------------\r\n",
      "\r\n",
      "ID  Truth(Spam/Ham : 1/0) Class(Spam/Ham : 1/0)\r\n",
      "-----------------------------------------------\r\n",
      "0001.2000-01-17.beck 0 0\r\n",
      "0018.1999-12-14.kaminski 0 0\r\n",
      "0004.2001-06-12.sa_and_hp 1 0\r\n",
      "0016.2003-12-19.gp 1 1\r\n",
      "0015.2001-07-05.sa_and_hp 1 0\r\n",
      "0009.1999-12-14.farmer 0 0\r\n",
      "0015.2001-02-12.kitchen 0 0\r\n",
      "0017.1999-12-14.kaminski 0 0\r\n",
      "0006.1999-12-13.kaminski 0 0\r\n",
      "0003.2000-01-17.beck 0 0\r\n",
      "0007.2004-08-01.bg 1 0\r\n",
      "0008.2001-06-12.sa_and_hp 1 0\r\n",
      "0007.2001-02-09.kitchen 0 0\r\n",
      "0008.2004-08-01.bg 1 0\r\n",
      "0010.1999-12-14.kaminski 0 0\r\n",
      "0015.2000-06-09.lokay 0 0\r\n",
      "0017.2003-12-18.gp 1 0\r\n",
      "0016.1999-12-15.farmer 0 0\r\n",
      "0012.2003-12-19.gp 1 0\r\n",
      "0012.2001-02-09.kitchen 0 0\r\n",
      "0005.1999-12-12.kaminski 0 1\r\n",
      "0007.2003-12-18.gp 1 0\r\n",
      "0014.2004-08-01.bg 1 0\r\n",
      "0006.2003-12-18.gp 1 0\r\n",
      "0011.1999-12-14.farmer 0 0\r\n",
      "0009.2001-02-09.kitchen 0 0\r\n",
      "0006.2001-02-08.kitchen 0 0\r\n",
      "0014.2001-07-04.sa_and_hp 1 0\r\n",
      "0010.1999-12-14.farmer 0 0\r\n",
      "0003.2004-08-01.bg 1 0\r\n",
      "0017.2004-08-01.bg 1 1\r\n",
      "0014.1999-12-14.kaminski 0 0\r\n",
      "0009.2001-06-26.sa_and_hp 1 0\r\n",
      "0013.2001-06-30.sa_and_hp 1 0\r\n",
      "0005.1999-12-14.farmer 0 0\r\n",
      "0010.2003-12-18.gp 1 0\r\n",
      "0013.1999-12-14.kaminski 0 0\r\n",
      "0001.2001-02-07.kitchen 0 0\r\n",
      "0008.2001-02-09.kitchen 0 0\r\n",
      "0016.2001-07-06.sa_and_hp 1 0\r\n",
      "0009.2003-12-18.gp 1 1\r\n",
      "0005.2000-06-06.lokay 0 0\r\n",
      "0001.2001-04-02.williams 0 0\r\n",
      "0011.2001-06-28.sa_and_hp 1 1\r\n",
      "0013.2004-08-01.bg 1 1\r\n",
      "0010.2004-08-01.bg 1 0\r\n",
      "0012.2000-06-08.lokay 0 0\r\n",
      "0006.2001-06-25.sa_and_hp 1 0\r\n",
      "0014.1999-12-15.farmer 0 0\r\n",
      "0009.2000-06-07.lokay 0 0\r\n",
      "0001.1999-12-10.farmer 0 0\r\n",
      "0008.2001-06-25.sa_and_hp 1 0\r\n",
      "0017.2001-04-03.williams 0 0\r\n",
      "0015.2003-12-19.gp 1 0\r\n",
      "0014.2001-02-12.kitchen 0 0\r\n",
      "0006.2004-08-01.bg 1 0\r\n",
      "0015.1999-12-15.farmer 0 0\r\n",
      "0016.2001-07-05.sa_and_hp 1 0\r\n",
      "0009.1999-12-13.kaminski 0 0\r\n",
      "0001.2000-06-06.lokay 0 0\r\n",
      "0002.2004-08-01.bg 1 1\r\n",
      "0002.1999-12-13.farmer 0 0\r\n",
      "0011.2003-12-18.gp 1 0\r\n",
      "0010.2001-06-28.sa_and_hp 1 1\r\n",
      "0004.1999-12-14.farmer 0 0\r\n",
      "0017.2004-08-02.bg 1 0\r\n",
      "0005.2001-06-23.sa_and_hp 1 0\r\n",
      "0007.1999-12-14.farmer 0 0\r\n",
      "0011.2001-06-29.sa_and_hp 1 0\r\n",
      "0008.2003-12-18.gp 1 0\r\n",
      "0007.2000-01-17.beck 0 0\r\n",
      "0003.1999-12-14.farmer 0 0\r\n",
      "0004.2004-08-01.bg 1 0\r\n",
      "0018.2003-12-18.gp 1 1\r\n",
      "0003.1999-12-10.kaminski 0 0\r\n",
      "0014.2003-12-19.gp 1 0\r\n",
      "0012.1999-12-14.farmer 0 0\r\n",
      "0003.2003-12-18.gp 1 0\r\n",
      "0004.1999-12-10.kaminski 0 1\r\n",
      "0018.2001-07-13.sa_and_hp 1 1\r\n",
      "0002.2001-02-07.kitchen 0 0\r\n",
      "0012.1999-12-14.kaminski 0 0\r\n",
      "0005.2003-12-18.gp 1 0\r\n",
      "0012.2000-01-17.beck 0 0\r\n",
      "0016.2004-08-01.bg 1 0\r\n",
      "0011.2004-08-01.bg 1 0\r\n",
      "0007.1999-12-13.kaminski 0 0\r\n",
      "0017.2000-01-17.beck 0 0\r\n",
      "0003.2001-02-08.kitchen 0 0\r\n",
      "0006.2001-04-03.williams 0 0\r\n",
      "0005.2001-02-08.kitchen 0 0\r\n",
      "0013.2001-04-03.williams 0 0\r\n",
      "0004.2001-04-02.williams 0 0\r\n",
      "0010.2001-02-09.kitchen 0 0\r\n",
      "0001.1999-12-10.kaminski 0 0\r\n",
      "0013.1999-12-14.farmer 0 0\r\n",
      "0015.1999-12-14.kaminski 0 0\r\n",
      "0016.2001-02-12.kitchen 0 0\r\n",
      "0002.2001-05-25.sa_and_hp 1 0\r\n",
      "0002.2003-12-18.gp 1 0\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present.\n",
    "\n",
    "   To do so, make sure that\n",
    "\n",
    "   * mapper.py counts all occurrences of all words, and\n",
    "   * reducer.py performs a word-distribution-wide Naive Bayes classification.\n",
    "\n",
    "In all cases, mapper.py will read in a portion of the email data, count some words and print out counts to a file.\n",
    "\n",
    "***NOTE:***\n",
    "Word Counts and other Intermediate results are not shown here (too long to list). This was just done by commenting code in the reducer.py and printing only some high level info and final classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5 \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output word counts and Classification using all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "-----------Counts for Naive Bayes Model -------------\r\n",
      "spam0wc 8054\r\n",
      "spam0dc 56\r\n",
      "spam1dc 44\r\n",
      "spam1wc 10666\r\n",
      "\r\n",
      "------Computed Probabilities from Training Set---\r\n",
      "P(prior_spam) = 0.44000\r\n",
      "P(prior_not_Spam) = 0.56000\r\n",
      "\r\n",
      "\r\n",
      "RESULTS: Classification of Test Data ----------\r\n",
      "\r\n",
      "ID  Truth(Spam/Ham : 1/0) Class(Spam/Ham : 1/0)\r\n",
      "-----------------------------------------------\r\n",
      "0001.2000-01-17.beck 0 0\r\n",
      "0018.1999-12-14.kaminski 0 0\r\n",
      "0004.2001-06-12.sa_and_hp 1 1\r\n",
      "0016.2003-12-19.gp 1 1\r\n",
      "0015.2001-07-05.sa_and_hp 1 1\r\n",
      "0009.1999-12-14.farmer 0 0\r\n",
      "0015.2001-02-12.kitchen 0 0\r\n",
      "0017.1999-12-14.kaminski 0 0\r\n",
      "0006.1999-12-13.kaminski 0 0\r\n",
      "0003.2000-01-17.beck 0 0\r\n",
      "0007.2004-08-01.bg 1 1\r\n",
      "0008.2001-06-12.sa_and_hp 1 1\r\n",
      "0007.2001-02-09.kitchen 0 0\r\n",
      "0008.2004-08-01.bg 1 1\r\n",
      "0010.1999-12-14.kaminski 0 0\r\n",
      "0015.2000-06-09.lokay 0 0\r\n",
      "0017.2003-12-18.gp 1 1\r\n",
      "0016.1999-12-15.farmer 0 0\r\n",
      "0012.2003-12-19.gp 1 1\r\n",
      "0012.2001-02-09.kitchen 0 0\r\n",
      "0005.1999-12-12.kaminski 0 0\r\n",
      "0007.2003-12-18.gp 1 1\r\n",
      "0014.2004-08-01.bg 1 1\r\n",
      "0006.2003-12-18.gp 1 1\r\n",
      "0011.1999-12-14.farmer 0 0\r\n",
      "0009.2001-02-09.kitchen 0 0\r\n",
      "0006.2001-02-08.kitchen 0 0\r\n",
      "0014.2001-07-04.sa_and_hp 1 1\r\n",
      "0010.1999-12-14.farmer 0 0\r\n",
      "0003.2004-08-01.bg 1 1\r\n",
      "0017.2004-08-01.bg 1 1\r\n",
      "0014.1999-12-14.kaminski 0 0\r\n",
      "0009.2001-06-26.sa_and_hp 1 1\r\n",
      "0013.2001-06-30.sa_and_hp 1 1\r\n",
      "0005.1999-12-14.farmer 0 0\r\n",
      "0010.2003-12-18.gp 1 1\r\n",
      "0013.1999-12-14.kaminski 0 0\r\n",
      "0001.2001-02-07.kitchen 0 0\r\n",
      "0008.2001-02-09.kitchen 0 0\r\n",
      "0016.2001-07-06.sa_and_hp 1 1\r\n",
      "0009.2003-12-18.gp 1 1\r\n",
      "0005.2000-06-06.lokay 0 0\r\n",
      "0001.2001-04-02.williams 0 0\r\n",
      "0011.2001-06-28.sa_and_hp 1 1\r\n",
      "0013.2004-08-01.bg 1 1\r\n",
      "0010.2004-08-01.bg 1 1\r\n",
      "0012.2000-06-08.lokay 0 0\r\n",
      "0006.2001-06-25.sa_and_hp 1 1\r\n",
      "0014.1999-12-15.farmer 0 0\r\n",
      "0009.2000-06-07.lokay 0 0\r\n",
      "0001.1999-12-10.farmer 0 0\r\n",
      "0008.2001-06-25.sa_and_hp 1 1\r\n",
      "0017.2001-04-03.williams 0 0\r\n",
      "0015.2003-12-19.gp 1 1\r\n",
      "0014.2001-02-12.kitchen 0 0\r\n",
      "0006.2004-08-01.bg 1 1\r\n",
      "0015.1999-12-15.farmer 0 0\r\n",
      "0016.2001-07-05.sa_and_hp 1 1\r\n",
      "0009.1999-12-13.kaminski 0 0\r\n",
      "0001.2000-06-06.lokay 0 0\r\n",
      "0002.2004-08-01.bg 1 1\r\n",
      "0002.1999-12-13.farmer 0 0\r\n",
      "0011.2003-12-18.gp 1 1\r\n",
      "0010.2001-06-28.sa_and_hp 1 1\r\n",
      "0004.1999-12-14.farmer 0 0\r\n",
      "0017.2004-08-02.bg 1 1\r\n",
      "0005.2001-06-23.sa_and_hp 1 1\r\n",
      "0007.1999-12-14.farmer 0 0\r\n",
      "0011.2001-06-29.sa_and_hp 1 1\r\n",
      "0008.2003-12-18.gp 1 1\r\n",
      "0007.2000-01-17.beck 0 0\r\n",
      "0003.1999-12-14.farmer 0 0\r\n",
      "0004.2004-08-01.bg 1 1\r\n",
      "0018.2003-12-18.gp 1 1\r\n",
      "0003.1999-12-10.kaminski 0 0\r\n",
      "0014.2003-12-19.gp 1 1\r\n",
      "0012.1999-12-14.farmer 0 0\r\n",
      "0003.2003-12-18.gp 1 1\r\n",
      "0004.1999-12-10.kaminski 0 0\r\n",
      "0018.2001-07-13.sa_and_hp 1 1\r\n",
      "0002.2001-02-07.kitchen 0 0\r\n",
      "0012.1999-12-14.kaminski 0 0\r\n",
      "0005.2003-12-18.gp 1 1\r\n",
      "0012.2000-01-17.beck 0 0\r\n",
      "0016.2004-08-01.bg 1 1\r\n",
      "0011.2004-08-01.bg 1 1\r\n",
      "0007.1999-12-13.kaminski 0 0\r\n",
      "0017.2000-01-17.beck 0 0\r\n",
      "0003.2001-02-08.kitchen 0 0\r\n",
      "0006.2001-04-03.williams 0 0\r\n",
      "0005.2001-02-08.kitchen 0 0\r\n",
      "0013.2001-04-03.williams 0 0\r\n",
      "0004.2001-04-02.williams 0 0\r\n",
      "0010.2001-02-09.kitchen 0 0\r\n",
      "0001.1999-12-10.kaminski 0 0\r\n",
      "0013.1999-12-14.farmer 0 0\r\n",
      "0015.1999-12-14.kaminski 0 0\r\n",
      "0016.2001-02-12.kitchen 0 0\r\n",
      "0002.2001-05-25.sa_and_hp 1 1\r\n",
      "0002.2003-12-18.gp 1 1\r\n",
      "\r\n",
      "Model Error Rate = 0.00000\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.6 Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes\n",
    "\n",
    "* Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "* Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error \n",
    "* Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error \n",
    "* Please prepare a table to present your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:***\n",
    "* The following code assumes \"enronemail_1h.txt\" is in the same folder as this notebook.\n",
    "* Results from HW1.5 are hard coded into the results data set for conveinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rates: Benchmark Comparison between Scikit Learn and HW1.5 \n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scikit MultinomialNB</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scikit BernoulliNB</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HW1.5 MulinomilNB</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy  Error\n",
       "0  Scikit MultinomialNB      1.00   0.00\n",
       "1    Scikit BernoulliNB      0.77   0.23\n",
       "2     HW1.5 MulinomilNB      1.00   0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "## \n",
    "## Author: Tigi Thomas\n",
    "## Description: Benchmark Classifier using Scikit Learn for HW 1.6\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "#fixed list of stop words\n",
    "stopwords = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "           'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','why','will','with','would','yet','you','your']\n",
    "\n",
    "#Use some pre-compiled re-gex for punctuation and numbers\n",
    "punctpattern = re.compile(r'[\\.\\^\\$\\*\\+\\-\\=\\{\\}\\[\\]\\\\\\|\\(\\)<>-@#%_=!:;,/\\'\\\"]')\n",
    "numpatt1 = re.compile(r'\\b[0-9]{2,100}\\b')\n",
    "numpatt2 = re.compile(r'[0-9]{2,100}\\b')\n",
    "numpatt3 = re.compile(r'\\b[0-9]{2,100}')\n",
    "stpwordspattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b')\n",
    "\n",
    "# Preprocess any text\n",
    "def preprocess_txt(s): \n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(punctpattern, r'',s)\n",
    "    s = re.sub(numpatt1, r'', s )\n",
    "    s = re.sub(numpatt2, r'', s )\n",
    "    s = re.sub(numpatt3, r'', s )\n",
    "    s = re.sub(stpwordspattern, r'',s)\n",
    "    return s\n",
    "\n",
    "email = []\n",
    "emailclass = []\n",
    "data = []\n",
    "\n",
    "# Read Data File and setup Prep Data for Scikit Learn Model\n",
    "filename = \"enronemail_1h.txt\"\n",
    "with open (filename, \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        line = line.lower()\n",
    "        emaildata = line.split(\"\\t\")\n",
    "        emaildatalen = len(emaildata)\n",
    "        \n",
    "        #don't really care about the first item, \n",
    "        #start from spam or not.. \n",
    "        if(emaildatalen >= 2):            \n",
    "            if(emaildatalen >= 3 ):\n",
    "                subject = emaildata[2]\n",
    "            else:\n",
    "                subject = \" \"\n",
    "            \n",
    "            if(emaildatalen == 4 ):\n",
    "                body = emaildata[3]\n",
    "            else:\n",
    "                body = \" \"\n",
    "                    \n",
    "            emailcontent = subject + \" \"  + body \n",
    "            \n",
    "            #pre-process to remove punctuation etc.\n",
    "            emailcontent = preprocess_txt(emailcontent).strip();\n",
    "            \n",
    "            data.append([emaildata[0], emailcontent])\n",
    "            email.append(emailcontent)\n",
    "            emailclass.append(emaildata[1])       \n",
    "                      \n",
    "\n",
    "# Shuffle and create Training and Test data Set- both are same for now.\n",
    "emails = np.array(data)\n",
    "X, Y = np.array(email), np.array(emailclass)\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "emails = emails[shuffle]\n",
    "\n",
    "#Both Test and Train are same data set as \n",
    "#per instructions in HW.\n",
    "train_data, train_labels  = X, Y\n",
    "test_data,  test_labels   = X, Y\n",
    "\n",
    "##checks\n",
    "#print emails.shape\n",
    "#print 'training label shape:', train_labels.shape\n",
    "#print 'test label shape:', test_labels.shape\n",
    "\n",
    "#Initialize Count Vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "#fit and transform\n",
    "fitX_train_counts = count_vect.fit_transform(train_data)\n",
    "fitX_test_counts  = count_vect.fit_transform(test_data)\n",
    "\n",
    "\n",
    "# Run thru the different Scikit Learn Models and Compare accuracy.\n",
    "\n",
    "print \"Error Rates: Benchmark Comparison between Scikit Learn and HW1.5 \"\n",
    "print \"----------------------------------------------------------------\"\n",
    "results = []\n",
    "\n",
    "# ----------------------- MultinomialNB --------------------------\n",
    "mnb = MultinomialNB() #(alpha = a)\n",
    "mnb.fit(fitX_train_counts, train_labels)\n",
    "\n",
    "# Predict using our Model\n",
    "mnb_predicted = mnb.predict(fitX_test_counts)\n",
    "accuracy  = round(np.where(mnb_predicted == test_labels, 1, 0).sum() / float(len(test_data)),5)\n",
    "error = round(1 - accuracy,5)\n",
    "#print 'Error Rate : Scikit-Learn Multinomia NB = {0:.5f}'.format(error)\n",
    "results.append(['Scikit MultinomialNB', accuracy, error])\n",
    "\n",
    "# ----------------------- BernoulliNB --------------------------\n",
    "bnb = BernoulliNB() #(alpha = a)\n",
    "bnb.fit(fitX_train_counts, train_labels)\n",
    "\n",
    "# Predict using our Model\n",
    "bnb_predicted = bnb.predict(fitX_test_counts)\n",
    "accuracy = round(np.where(bnb_predicted == test_labels, 1, 0).sum() / float(len(test_data)),5)\n",
    "error = round(1 - accuracy,5)\n",
    "#print 'Error Rate : Sciki-Learn Bernoulli NB = {0:.5f}'.format(error)\n",
    "results.append(['Scikit BernoulliNB', accuracy, error])\n",
    "\n",
    "# ----------------------- HW1.5 --------------------------\n",
    "#Add in error rate from HW1.5 above\n",
    "#print 'Error Rate : HW1.5 MulinomilNB = {0:.5f}'.format(0.0)\n",
    "results.append(['HW1.5 MulinomilNB', 1.0, 0.0])\n",
    "\n",
    "# Tabularize the Results\n",
    "results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Error\"])\n",
    "display(results)\n",
    "#HTML(results.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)\n",
    "\n",
    "    When using both Scikit MultinomialNB and MultinomialNB fro m HW1.5, the error rates were zero. In both cases the both the entire data set was used to train and to test the model. Additionally, Smoothing was used in HW1.5, and the default Scikit MultinomialNB 'alpha' (Additive Laplace smoothing parameter (0 for no smoothing)) is 1.0 , resulting in similar results.\n",
    "\n",
    "##Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn\n",
    "\n",
    "    BernoulliNB implements the naive Bayes assuming each feature to be a binary-valued (Bernoulli, boolean) variable. Therefore samples are required to be represented as binary-valued feature vectors. The decision rule for Bernoulli NB explicitly penalizes non-occurrence of a feature versus MultinomialNB which ignores a non-occurring feature. Since the feature vectors are now yes or no vectors versus count vectors, with larger documents there is more chance of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
