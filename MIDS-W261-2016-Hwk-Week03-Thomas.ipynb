{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Section 3\n",
    "## Homework, Week 3\n",
    "## Name: T.Thomas\n",
    "## Email: tgthomas@berkeley.edu\n",
    "## Submission Date:  ****** PST\n",
    "\n",
    "### nbviewer link:\n",
    "\n",
    "\n",
    "### pdf link:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 ASSIGNMENTS using Hadoop Streaming and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference Links\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a merge sort? Where is it used in Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer science, merge sort (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. \n",
    "\n",
    "Merge sort is a comparison based recursive sorting algorithm.\n",
    "Conceptually there are two steps :\n",
    "\n",
    "   1. Divide the unsorted list into **n** sublists, each containing 1 element (base caase is a list of one element, and it is considered sorted).\n",
    "   2. Repeatedly merge sublists to generate new sorted sublists until 1 sublist remains which will be the sorted list.\n",
    "\n",
    "\n",
    "<img src=\"http://interactivepython.org/runestone/static/pythonds/_images/mergesortB.png\" height=\"300\" width=\"300\" align=right  hspace=100 vspace=10 border=100>\n",
    "<img src=\"http://interactivepython.org/runestone/static/pythonds/_images/mergesortA.png\" height=\"300\" width=\"300\" align=right  hspace=100 vspace=10 border=100>\n",
    "\n",
    "\n",
    "\n",
    "**Source:* http://interactivepython.org/runestone/static/pythonds/SortSearch/TheMergeSort.html*\n",
    "\n",
    "\n",
    "\n",
    "It turns out that sorting partially sorted lists is much more efficient in terms of operations and memory consumption than sorting the complete list.\n",
    "\n",
    "If the reducer gets 4 sorted lists it only needs to look for the smallest element of the 4 lists and pick that one. If the number of lists is constant this reducing is an O(N) operation.\n",
    "\n",
    "Also typically the reducers are also \"distributed\" in something like a tree, so the work can be parrallelized too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is  a combiner function in the context of Hadoop? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Combiner Functions\n",
    "Many MapReduce jobs are limited by the bandwidth available on the cluster, so it pays to minimize the data transferred between map and reduce tasks. Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner function’s output forms the input to the reduce function. Because the combiner function is an optimization, Hadoop does not provide a guarantee of how many times it will call it for a particular map output record, if at all. In other words, calling the combiner function zero, one, or many times should produce the same output from the reducer.\n",
    "\n",
    "Running the combiner function makes for a more compact map output, so there is less data to write to local disk and to transfer to the reducer.\n",
    "\n",
    "\n",
    "If a combiner function is used, then it has the same form as the reduce function (and is an implementation of Reducer), except its output types are the intermediate key and value types (K2 and V2), so they can feed the reduce function:\n",
    "\n",
    "    map: $(K1, V1) → list(K2, V2)$\n",
    "    combiner: $(K2, list(V2)) → list(K2, V2)$\n",
    "    reduce: $(K2, list(V2)) → list(K3, V3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give an example where it can be used and justify why it should be used in the context of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle is the main heart of MapReduce\n",
    "\n",
    "The Mapper and Reducer's have a contract that all similar keys go to the same reducer. This means that after the mapper is done, there needs to be an intermediate sorting step where the intermediate map results are sorted to meet this inherent contract.\n",
    "\n",
    "This process by which the system performs the sort and transfers the map outputs to the reducers as inputs is known as the shuffle.\n",
    "\n",
    "The MR framework accomplishes this in 3 steps \n",
    "\t- Partition\n",
    "\t- Sortreal\n",
    "\t- Combine (in memory If possible or on disk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "\n",
    "    Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "    1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "    1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "    1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "    1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "### User-defined Counters\n",
    "\n",
    "#### Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "#### Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper to test counters in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting counter_test_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile counter_test_mapper.py\n",
    "#!/usr/bin/python\n",
    "## counter_test_mapper.py\n",
    "## Author: Tigi Thomas\n",
    "## Description: mapper code for HW 3.1 \n",
    "import csv\n",
    "import sys\n",
    "\n",
    "def printcounters(counters):\n",
    "    for counter, total in counters.iteritems():\n",
    "        sys.stderr.write(\"reporter:counter:ComplaintCategory,{0},{1}\\n\".format(counter,total))\n",
    "        \n",
    "counters = {} #keep all the counters.\n",
    "header_done = False #Just keep tracks if we are done reading the header\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for row in csv.reader(iter(sys.stdin.readline, '')):\n",
    "    if not header_done:\n",
    "        header_done = True;\n",
    "    else:\n",
    "        category = row[1].lower()\n",
    "        if(category == 'debt collection' or category == 'mortgage'):\n",
    "            counters[row[1]] = counters.get(row[1], 0) + 1\n",
    "        else:\n",
    "            counters[\"Other\"] = counters.get(\"Other\", 0) + 1\n",
    "        print row[1] + '\\t1'\n",
    "\n",
    "printcounters(counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer with counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount_reducer.py\n",
    "#!/usr/bin/python\n",
    "## basic_reducer.py\n",
    "##     - This is a very basic reducer to reduce\n",
    "##       Word,count\n",
    "## Author: Tigi Thomas\n",
    "## Description: reducer code for HW 3.1 \n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "#This custom counter , counts how many reducers.\n",
    "sys.stderr.write(\"reporter:counter:HowMany,Reducer,1\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Mapper & Reducer using unix commandline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "#!cat Consumer_Complaints.csv | python counter_test_mapper.py | sort -k1,1 | python wordcount_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run MapReduce Job via Hadoop Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check contents of hdfs folder.\n",
    "#!hdfs dfs -ls /user/dsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning out old input and output files...\n",
      "------------------------------------------\n",
      "\n",
      "16/01/31 19:32:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 19:32:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/Consumer_Complaints.csv\n",
      "16/01/31 19:32:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 19:32:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/countertestoutput/_SUCCESS\n",
      "16/01/31 19:32:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/countertestoutput/part-00000\n",
      "16/01/31 19:32:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/countertestoutput/part-00001\n",
      "16/01/31 19:32:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/countertestoutput/part-00002\n",
      "16/01/31 19:32:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 19:32:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Folder already exists\n",
    "#!hdfs dfs -mkdir -p /user/dsq\n",
    "\n",
    "#Clear up prev inputs/outputs.\n",
    "!echo\n",
    "!echo \"Cleaning out old input and output files...\"\n",
    "!echo \"------------------------------------------\"\n",
    "!echo\n",
    "!hdfs dfs -rm /user/dsq/Consumer_Complaints.csv\n",
    "!hdfs dfs -rm /user/dsq/countertestoutput/*\n",
    "!hdfs dfs -rmdir /user/dsq/countertestoutput\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 19:32:52 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 19:32:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [counter_test_mapper.py, counter_test_reducer.py, Consumer_Complaints.csv] [] /tmp/streamjob8197482647832701843.jar tmpDir=null\n",
      "16/01/31 19:32:55 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 19:32:55 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 19:32:55 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 19:32:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 19:32:56 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 19:32:56 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/01/31 19:32:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1028387688_0001\n",
      "16/01/31 19:32:56 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/counter_test_mapper.py as file:/app/hadoop/tmp/mapred/local/1454297576663/counter_test_mapper.py\n",
      "16/01/31 19:32:57 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/counter_test_reducer.py as file:/app/hadoop/tmp/mapred/local/1454297576664/counter_test_reducer.py\n",
      "16/01/31 19:32:57 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/Consumer_Complaints.csv as file:/app/hadoop/tmp/mapred/local/1454297576665/Consumer_Complaints.csv\n",
      "16/01/31 19:32:57 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 19:32:57 INFO mapreduce.Job: Running job: job_local1028387688_0001\n",
      "16/01/31 19:32:57 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 19:32:57 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 19:32:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:32:57 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 19:32:57 INFO mapred.LocalJobRunner: Starting task: attempt_local1028387688_0001_m_000000_0\n",
      "16/01/31 19:32:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:32:57 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 19:32:57 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/dsq/Consumer_Complaints.csv:0+50906486\n",
      "16/01/31 19:32:57 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/01/31 19:32:57 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 19:32:57 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 19:32:57 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 19:32:57 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 19:32:57 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 19:32:57 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 19:32:57 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./counter_test_mapper.py]\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 19:32:57 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 19:32:58 INFO mapreduce.Job: Job job_local1028387688_0001 running in uber mode : false\n",
      "16/01/31 19:32:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 19:32:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:32:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:32:58 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:32:58 INFO streaming.PipeMapRed: Records R/W=779/1\n",
      "16/01/31 19:32:58 INFO streaming.PipeMapRed: R/W/S=1000/283/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:32:59 INFO streaming.PipeMapRed: R/W/S=10000/9231/0 in:10000=10000/1 [rec/s] out:9231=9231/1 [rec/s]\n",
      "16/01/31 19:32:59 INFO streaming.PipeMapRed: R/W/S=100000/99421/0 in:100000=100000/1 [rec/s] out:99421=99421/1 [rec/s]\n",
      "16/01/31 19:33:00 INFO streaming.PipeMapRed: R/W/S=200000/198558/0 in:100000=200000/2 [rec/s] out:99279=198558/2 [rec/s]\n",
      "16/01/31 19:33:01 INFO streaming.PipeMapRed: R/W/S=300000/299139/0 in:100000=300000/3 [rec/s] out:99713=299139/3 [rec/s]\n",
      "16/01/31 19:33:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 19:33:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 19:33:01 INFO mapred.LocalJobRunner: \n",
      "16/01/31 19:33:01 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 19:33:01 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 19:33:01 INFO mapred.MapTask: bufstart = 0; bufend = 4878312; bufvoid = 104857600\n",
      "16/01/31 19:33:01 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24962752(99851008); length = 1251645/6553600\n",
      "16/01/31 19:33:01 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 19:33:01 INFO mapred.Task: Task:attempt_local1028387688_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 19:33:01 INFO mapred.LocalJobRunner: Records R/W=779/1\n",
      "16/01/31 19:33:01 INFO mapred.Task: Task 'attempt_local1028387688_0001_m_000000_0' done.\n",
      "16/01/31 19:33:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1028387688_0001_m_000000_0\n",
      "16/01/31 19:33:01 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 19:33:01 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 19:33:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1028387688_0001_r_000000_0\n",
      "16/01/31 19:33:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:33:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 19:33:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@27f55b38\n",
      "16/01/31 19:33:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 19:33:01 INFO reduce.EventFetcher: attempt_local1028387688_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 19:33:01 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1028387688_0001_m_000000_0 decomp: 3644332 len: 3644336 to MEMORY\n",
      "16/01/31 19:33:01 INFO reduce.InMemoryMapOutput: Read 3644332 bytes from map-output for attempt_local1028387688_0001_m_000000_0\n",
      "16/01/31 19:33:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 3644332, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3644332\n",
      "16/01/31 19:33:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 19:33:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:33:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 19:33:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:33:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3644306 bytes\n",
      "16/01/31 19:33:02 INFO reduce.MergeManagerImpl: Merged 1 segments, 3644332 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 19:33:02 INFO reduce.MergeManagerImpl: Merging 1 files, 3644336 bytes from disk\n",
      "16/01/31 19:33:02 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 19:33:02 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:33:02 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3644306 bytes\n",
      "16/01/31 19:33:02 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:33:02 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./counter_test_reducer.py]\n",
      "16/01/31 19:33:02 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 19:33:02 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 19:33:02 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:02 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:02 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:02 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:02 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:02 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 19:33:02 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: Records R/W=174654/1\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 19:33:03 INFO mapred.Task: Task:attempt_local1028387688_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 19:33:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:33:03 INFO mapred.Task: Task attempt_local1028387688_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 19:33:03 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1028387688_0001_r_000000_0' to hdfs://localhost:54310/user/dsq/countertestoutput/_temporary/0/task_local1028387688_0001_r_000000\n",
      "16/01/31 19:33:03 INFO mapred.LocalJobRunner: Records R/W=174654/1 > reduce\n",
      "16/01/31 19:33:03 INFO mapred.Task: Task 'attempt_local1028387688_0001_r_000000_0' done.\n",
      "16/01/31 19:33:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local1028387688_0001_r_000000_0\n",
      "16/01/31 19:33:03 INFO mapred.LocalJobRunner: Starting task: attempt_local1028387688_0001_r_000001_0\n",
      "16/01/31 19:33:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:33:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 19:33:03 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@9992770\n",
      "16/01/31 19:33:03 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 19:33:03 INFO reduce.EventFetcher: attempt_local1028387688_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 19:33:03 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1028387688_0001_m_000000_0 decomp: 1859808 len: 1859812 to MEMORY\n",
      "16/01/31 19:33:03 INFO reduce.InMemoryMapOutput: Read 1859808 bytes from map-output for attempt_local1028387688_0001_m_000000_0\n",
      "16/01/31 19:33:03 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1859808, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1859808\n",
      "16/01/31 19:33:03 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 19:33:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:33:03 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 19:33:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:33:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1859792 bytes\n",
      "16/01/31 19:33:03 INFO reduce.MergeManagerImpl: Merged 1 segments, 1859808 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 19:33:03 INFO reduce.MergeManagerImpl: Merging 1 files, 1859812 bytes from disk\n",
      "16/01/31 19:33:03 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 19:33:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:33:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1859792 bytes\n",
      "16/01/31 19:33:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./counter_test_reducer.py]\n",
      "16/01/31 19:33:03 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:33:03 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: Records R/W=138258/1\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 19:33:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 19:33:03 INFO mapred.Task: Task:attempt_local1028387688_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/01/31 19:33:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:33:03 INFO mapred.Task: Task attempt_local1028387688_0001_r_000001_0 is allowed to commit now\n",
      "16/01/31 19:33:03 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1028387688_0001_r_000001_0' to hdfs://localhost:54310/user/dsq/countertestoutput/_temporary/0/task_local1028387688_0001_r_000001\n",
      "16/01/31 19:33:03 INFO mapred.LocalJobRunner: Records R/W=138258/1 > reduce\n",
      "16/01/31 19:33:03 INFO mapred.Task: Task 'attempt_local1028387688_0001_r_000001_0' done.\n",
      "16/01/31 19:33:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local1028387688_0001_r_000001_0\n",
      "16/01/31 19:33:03 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 19:33:04 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 19:33:04 INFO mapreduce.Job: Job job_local1028387688_0001 completed successfully\n",
      "16/01/31 19:33:04 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=188209078\n",
      "\t\tFILE: Number of bytes written=197733224\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=152719458\n",
      "\t\tHDFS: Number of bytes written=295\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=4878312\n",
      "\t\tMap output materialized bytes=5504148\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce shuffle bytes=5504148\n",
      "\t\tReduce input records=312912\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=625824\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=621\n",
      "\t\tTotal committed heap usage (bytes)=1140326400\n",
      "\tComplaintCategory\n",
      "\t\tDebt collection=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOther=142788\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=184\n",
      "16/01/31 19:33:04 INFO streaming.StreamJob: Output directory: countertestoutput\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.maps=2 \\\n",
    "-D mapreduce.job.reduces=2 \\\n",
    "-D mapred.job.name=\"Testing Counters\" \\\n",
    "-mapper counter_test_mapper.py -file counter_test_mapper.py \\\n",
    "-reducer wordcount_reducer.py -file wordcount_reducer.py \\\n",
    "-input Consumer_Complaints.csv -file Consumer_Complaints.csv \\\n",
    "-output countertestoutput\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***From Log***\n",
    "\n",
    "    ComplaintCategory\n",
    "\t\tDebt collection=44372\n",
    "\t\tMortgage=125752\n",
    "\t\tOther=142788"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "\n",
    "   **foo foo quux labs foo bar quux**\n",
    "\n",
    "* Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. \n",
    "* What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. \n",
    "\n",
    "The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "\n",
    "* **3.2.a** Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "* **3.2.b** Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.\n",
    "\n",
    "* **3.2.c** Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2.a) Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing howmanymrinput.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile howmanymrinput.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting howmany_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile howmany_mapper.py\n",
    "#!/usr/bin/python\n",
    "## howmany_mapper.py\n",
    "##   - simple mapper with mapper level counter to see \n",
    "##     how many mappers are created.\n",
    "## Author: Tigi Thomas\n",
    "## Description: mapper code for HW 3.2 \n",
    "\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HowMany,Mapper,1\\n\")\n",
    "    \n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer : We use the simple Wordcount Reducer from HW 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generete the sorted output just with the reducer\n",
    "#!cat howmanymrinput.txt | python howmany_mapper.py | sort -k1,1  | python wordcount_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning out old input and output files...\n",
      "------------------------------------------\n",
      "\n",
      "16/01/31 19:37:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 19:37:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/howmanymrinput.txt\n",
      "16/01/31 19:37:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 19:37:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/howmanymroutput/_SUCCESS\n",
      "16/01/31 19:37:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/howmanymroutput/part-00000\n",
      "16/01/31 19:37:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/howmanymroutput/part-00001\n",
      "16/01/31 19:37:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/howmanymroutput/part-00002\n",
      "16/01/31 19:37:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/howmanymroutput/part-00003\n",
      "16/01/31 19:37:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 19:37:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Create folder if needed (already exists)\n",
    "#!hdfs dfs -mkdir -p /user/dsq\n",
    "\n",
    "#Clear up prev inputs/outputs.\n",
    "!echo\n",
    "!echo \"Cleaning out old input and output files...\"\n",
    "!echo \"------------------------------------------\"\n",
    "!echo\n",
    "!hdfs dfs -rm /user/dsq/howmanymrinput.txt\n",
    "!hdfs dfs -rm /user/dsq/howmanymroutput/*\n",
    "!hdfs dfs -rmdir /user/dsq/howmanymroutput\n",
    "!hdfs dfs -put howmanymrinput.txt /user/dsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Hadoop Map Reducer for 3.2.a...\n",
      "------------------------------------------\n",
      "\n",
      "16/01/31 19:41:11 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 19:41:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [howmany_mapper.py, howmany_reducer.py, howmanymrinput.txt] [] /tmp/streamjob8442593972214384666.jar tmpDir=null\n",
      "16/01/31 19:41:12 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 19:41:12 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 19:41:12 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 19:41:13 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 19:41:13 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 19:41:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1662265678_0001\n",
      "16/01/31 19:41:14 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/howmany_mapper.py as file:/app/hadoop/tmp/mapred/local/1454298073701/howmany_mapper.py\n",
      "16/01/31 19:41:14 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/howmany_reducer.py as file:/app/hadoop/tmp/mapred/local/1454298073702/howmany_reducer.py\n",
      "16/01/31 19:41:14 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/howmanymrinput.txt as file:/app/hadoop/tmp/mapred/local/1454298073703/howmanymrinput.txt\n",
      "16/01/31 19:41:14 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 19:41:14 INFO mapreduce.Job: Running job: job_local1662265678_0001\n",
      "16/01/31 19:41:14 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 19:41:14 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 19:41:14 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:41:14 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 19:41:14 INFO mapred.LocalJobRunner: Starting task: attempt_local1662265678_0001_m_000000_0\n",
      "16/01/31 19:41:14 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:41:14 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/dsq/howmanymrinput.txt:0+30\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 19:41:14 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./howmany_mapper.py]\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 19:41:14 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 19:41:14 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:41:14 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/01/31 19:41:14 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 19:41:14 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 19:41:14 INFO mapred.LocalJobRunner: \n",
      "16/01/31 19:41:14 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: bufstart = 0; bufend = 45; bufvoid = 104857600\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/01/31 19:41:14 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 19:41:14 INFO mapred.Task: Task:attempt_local1662265678_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 19:41:14 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/01/31 19:41:14 INFO mapred.Task: Task 'attempt_local1662265678_0001_m_000000_0' done.\n",
      "16/01/31 19:41:14 INFO mapred.LocalJobRunner: Finishing task: attempt_local1662265678_0001_m_000000_0\n",
      "16/01/31 19:41:14 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 19:41:14 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 19:41:14 INFO mapred.LocalJobRunner: Starting task: attempt_local1662265678_0001_r_000000_0\n",
      "16/01/31 19:41:14 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:41:14 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 19:41:14 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@676e582a\n",
      "16/01/31 19:41:14 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 19:41:14 INFO reduce.EventFetcher: attempt_local1662265678_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 19:41:15 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1662265678_0001_m_000000_0 decomp: 20 len: 24 to MEMORY\n",
      "16/01/31 19:41:15 INFO reduce.InMemoryMapOutput: Read 20 bytes from map-output for attempt_local1662265678_0001_m_000000_0\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->20\n",
      "16/01/31 19:41:15 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merged 1 segments, 20 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merging 1 files, 24 bytes from disk\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./howmany_reducer.py]\n",
      "16/01/31 19:41:15 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 19:41:15 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: Records R/W=2/1\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 19:41:15 INFO mapreduce.Job: Job job_local1662265678_0001 running in uber mode : false\n",
      "16/01/31 19:41:15 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 19:41:15 INFO mapred.Task: Task:attempt_local1662265678_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO mapred.Task: Task attempt_local1662265678_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 19:41:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1662265678_0001_r_000000_0' to hdfs://localhost:54310/user/dsq/howmanymroutput/_temporary/0/task_local1662265678_0001_r_000000\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n",
      "16/01/31 19:41:15 INFO mapred.Task: Task 'attempt_local1662265678_0001_r_000000_0' done.\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local1662265678_0001_r_000000_0\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: Starting task: attempt_local1662265678_0001_r_000001_0\n",
      "16/01/31 19:41:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:41:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 19:41:15 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@131d15b6\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 19:41:15 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1662265678_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
      "16/01/31 19:41:15 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local1662265678_0001_m_000000_0\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
      "16/01/31 19:41:15 INFO reduce.EventFetcher: attempt_local1662265678_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20 bytes\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20 bytes\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./howmany_reducer.py]\n",
      "16/01/31 19:41:15 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 19:41:15 INFO mapred.Task: Task:attempt_local1662265678_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO mapred.Task: Task attempt_local1662265678_0001_r_000001_0 is allowed to commit now\n",
      "16/01/31 19:41:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1662265678_0001_r_000001_0' to hdfs://localhost:54310/user/dsq/howmanymroutput/_temporary/0/task_local1662265678_0001_r_000001\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/01/31 19:41:15 INFO mapred.Task: Task 'attempt_local1662265678_0001_r_000001_0' done.\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local1662265678_0001_r_000001_0\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: Starting task: attempt_local1662265678_0001_r_000002_0\n",
      "16/01/31 19:41:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:41:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 19:41:15 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1e2161df\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 19:41:15 INFO reduce.EventFetcher: attempt_local1662265678_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 19:41:15 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1662265678_0001_m_000000_0 decomp: 10 len: 14 to MEMORY\n",
      "16/01/31 19:41:15 INFO reduce.InMemoryMapOutput: Read 10 bytes from map-output for attempt_local1662265678_0001_m_000000_0\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->10\n",
      "16/01/31 19:41:15 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merged 1 segments, 10 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merging 1 files, 14 bytes from disk\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./howmany_reducer.py]\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 19:41:15 INFO mapred.Task: Task:attempt_local1662265678_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO mapred.Task: Task attempt_local1662265678_0001_r_000002_0 is allowed to commit now\n",
      "16/01/31 19:41:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1662265678_0001_r_000002_0' to hdfs://localhost:54310/user/dsq/howmanymroutput/_temporary/0/task_local1662265678_0001_r_000002\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/01/31 19:41:15 INFO mapred.Task: Task 'attempt_local1662265678_0001_r_000002_0' done.\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local1662265678_0001_r_000002_0\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: Starting task: attempt_local1662265678_0001_r_000003_0\n",
      "16/01/31 19:41:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:41:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 19:41:15 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2c4c9e88\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 19:41:15 INFO reduce.EventFetcher: attempt_local1662265678_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 19:41:15 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local1662265678_0001_m_000000_0 decomp: 11 len: 15 to MEMORY\n",
      "16/01/31 19:41:15 INFO reduce.InMemoryMapOutput: Read 11 bytes from map-output for attempt_local1662265678_0001_m_000000_0\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->11\n",
      "16/01/31 19:41:15 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merged 1 segments, 11 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merging 1 files, 15 bytes from disk\n",
      "16/01/31 19:41:15 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 19:41:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/01/31 19:41:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./howmany_reducer.py]\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 19:41:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 19:41:16 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/01/31 19:41:16 INFO mapred.Task: Task:attempt_local1662265678_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/01/31 19:41:16 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 19:41:16 INFO mapred.Task: Task attempt_local1662265678_0001_r_000003_0 is allowed to commit now\n",
      "16/01/31 19:41:16 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1662265678_0001_r_000003_0' to hdfs://localhost:54310/user/dsq/howmanymroutput/_temporary/0/task_local1662265678_0001_r_000003\n",
      "16/01/31 19:41:16 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/01/31 19:41:16 INFO mapred.Task: Task 'attempt_local1662265678_0001_r_000003_0' done.\n",
      "16/01/31 19:41:16 INFO mapred.LocalJobRunner: Finishing task: attempt_local1662265678_0001_r_000003_0\n",
      "16/01/31 19:41:16 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 19:41:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 19:41:17 INFO mapreduce.Job: Job job_local1662265678_0001 completed successfully\n",
      "16/01/31 19:41:17 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=20301\n",
      "\t\tFILE: Number of bytes written=1420794\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=150\n",
      "\t\tHDFS: Number of bytes written=65\n",
      "\t\tHDFS: Number of read operations=55\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=25\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=34\n",
      "\t\tTotal committed heap usage (bytes)=1355284480\n",
      "\tHowMany\n",
      "\t\tMapper=1\n",
      "\t\tReducer=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/01/31 19:41:17 INFO streaming.StreamJob: Output directory: howmanymroutput\n"
     ]
    }
   ],
   "source": [
    "!echo\n",
    "!echo \"Running Hadoop Map Reducer for 3.2...\"\n",
    "!echo \"------------------------------------------\"\n",
    "!echo\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-D mapreduce.job.reduces=4 \\\n",
    "-D mapreduce.job.name=\"Testing Counters How Many Mappers and Reducers 3.2.a\" \\\n",
    "-mapper howmany_mapper.py -file howmany_mapper.py \\\n",
    "-reducer howmany_reducer.py -file wordcount_reducer.py \\\n",
    "-input howmanymrinput.txt -file howmanymrinput.txt \\\n",
    "-output howmanymroutput \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.a Number of Mappers and Reducers    \n",
    "\n",
    "    HowMany\n",
    "\t\tMapper=1\n",
    "\t\tReducer=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (3.2.a) Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting issuecount_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile issuecount_mapper.py\n",
    "#!/usr/bin/python\n",
    "## issuecount_mapper.py\n",
    "##    - mapper for word & count from \n",
    "##      issue field in the Complaints dataset.csv\n",
    "## Author: Tigi Thomas\n",
    "## Description: mapper code for HW 3.2.a\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "#This will be our mapper count.\n",
    "sys.stderr.write(\"reporter:counter:HowMany,Mapper,1\\n\")\n",
    "\n",
    "#fixed list of stop words\n",
    "# ---------------------------------------\n",
    "#Not used...\n",
    "'''\n",
    "with open (stopwords, \"rU\") as stopwordf:\n",
    "     for line in stopwordf:\n",
    "        line = line.lower()\n",
    "        stopwords = line.split(\",\")\n",
    "'''\n",
    "# --------------------------------------\n",
    "\n",
    "#'''\n",
    "# Stop words to use \n",
    "stopwords = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "             'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','whya','will','with','would','yet','you','your']\n",
    "#'''\n",
    "\n",
    "#Use some pre-compiled re-gex for punctuation and numbers (exclude period and comma)\n",
    "punctpattern = re.compile(r'[\\.\\^\\$\\*\\+\\-\\=\\{\\}\\[\\]\\\\\\|\\(\\)<>-@&#%_=!?:;,/\\'\\\"]')\n",
    "#since we are going to consider period and comma as delimitter in addition to space\n",
    "nonprintable = re.compile('[^\\s!-~]')\n",
    "#take out numbers..\n",
    "numpatt1 = re.compile(r'\\b[0-9]{1,100}\\b')\n",
    "numpatt2 = re.compile(r'[0-9]{1,100}\\b')\n",
    "numpatt3 = re.compile(r'\\b[0-9]{1,100}')\n",
    "\n",
    "# Preprocess any text to remove punctuations, numbers, convert to lower etc.\n",
    "def preprocess_txt(s): \n",
    "    s = s.lower()\n",
    "    s = re.sub(nonprintable, r' ', s)\n",
    "    s = re.sub(punctpattern, r' ', s)\n",
    "    s = re.sub(numpatt1, r'', s )\n",
    "    s = re.sub(numpatt2, r'', s )\n",
    "    s = re.sub(numpatt3, r'', s )\n",
    "    return s\n",
    "\n",
    "# Print a set of counters...\n",
    "def printcounters(counters):\n",
    "    for counter, total in counters.iteritems():\n",
    "        sys.stderr.write(\"reporter:counter:ComplaintCategory,{0},{1}\\n\".format(counter,total))\n",
    "\n",
    "#Just keep tracks if we are done reading the header\n",
    "header_done = False \n",
    "total_words = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "#   since the data is comma separated, we use the csv.reader\n",
    "#   to get worry free comma delimitted values , even those\n",
    "#   inside quotes.\n",
    "for row in csv.reader(iter(sys.stdin.readline, '')):\n",
    "    if not header_done:\n",
    "        header_done = True;\n",
    "    else:\n",
    "        \n",
    "        # issue text is in the 4th field.\n",
    "        # remove leading and trailing whitespace\n",
    "        issuelog = row[3].strip()\n",
    "\n",
    "        # pre-process the line\n",
    "        issuelog = preprocess_txt(issuelog)\n",
    "\n",
    "        # split the line into words\n",
    "        words = issuelog.split()\n",
    "\n",
    "        #filter out the stop words\n",
    "        filtered_words = [tk for tk in words if tk not in stopwords]\n",
    "\n",
    "        # increase counters\n",
    "        for word in filtered_words:\n",
    "            # write the results to STDOUT \n",
    "            # tab-delimited; the trivial word count is 1\n",
    "            total_words += 1\n",
    "            print word + '\\t1'\n",
    "\n",
    "print '**total\\t{0}'.format(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer : We use the simple Wordcount Reducer from HW 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning out old input and output files...\n",
      "------------------------------------------\n",
      "\n",
      "16/01/31 19:53:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 19:53:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/Consumer_Complaints.csv\n",
      "16/01/31 19:53:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 19:53:39 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/issuecountoutput/_SUCCESS\n",
      "16/01/31 19:53:39 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/issuecountoutput/part-00000\n",
      "16/01/31 19:53:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 19:53:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Clear up prev inputs/outputs.\n",
    "!echo\n",
    "!echo \"Cleaning out old input and output files...\"\n",
    "!echo \"------------------------------------------\"\n",
    "!echo\n",
    "!hdfs dfs -rm /user/dsq/Consumer_Complaints.csv\n",
    "!hdfs dfs -rm /user/dsq/issuecountoutput/*\n",
    "!hdfs dfs -rmdir /user/dsq/issuecountoutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Hadoop Map Reducer for 3.2.b...\n",
      "------------------------------------------\n",
      "\n",
      "16/01/31 19:59:48 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 19:59:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [issuecount_mapper.py, wordcount_reducer.py, Consumer_Complaints.csv] [] /tmp/streamjob4828918602925064173.jar tmpDir=null\n",
      "16/01/31 19:59:50 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 19:59:51 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 19:59:51 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 19:59:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 19:59:51 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 19:59:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1383108653_0001\n",
      "16/01/31 19:59:52 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/issuecount_mapper.py as file:/app/hadoop/tmp/mapred/local/1454299192260/issuecount_mapper.py\n",
      "16/01/31 19:59:52 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/wordcount_reducer.py as file:/app/hadoop/tmp/mapred/local/1454299192261/wordcount_reducer.py\n",
      "16/01/31 19:59:52 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/Consumer_Complaints.csv as file:/app/hadoop/tmp/mapred/local/1454299192262/Consumer_Complaints.csv\n",
      "16/01/31 19:59:53 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 19:59:53 INFO mapreduce.Job: Running job: job_local1383108653_0001\n",
      "16/01/31 19:59:53 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 19:59:53 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 19:59:53 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:59:53 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 19:59:53 INFO mapred.LocalJobRunner: Starting task: attempt_local1383108653_0001_m_000000_0\n",
      "16/01/31 19:59:53 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 19:59:53 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 19:59:53 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/dsq/Consumer_Complaints.csv:0+50906486\n",
      "16/01/31 19:59:53 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/01/31 19:59:53 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 19:59:53 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 19:59:53 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 19:59:53 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 19:59:53 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 19:59:53 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 19:59:53 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./issuecount_mapper.py]\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 19:59:53 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 19:59:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:59:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:59:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:59:53 INFO streaming.PipeMapRed: Records R/W=779/1\n",
      "16/01/31 19:59:53 INFO streaming.PipeMapRed: R/W/S=1000/437/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:59:54 INFO streaming.PipeMapRed: R/W/S=10000/37305/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 19:59:54 INFO mapreduce.Job: Job job_local1383108653_0001 running in uber mode : false\n",
      "16/01/31 19:59:54 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 19:59:56 INFO streaming.PipeMapRed: R/W/S=100000/388560/0 in:50000=100000/2 [rec/s] out:194280=388560/2 [rec/s]\n",
      "16/01/31 19:59:59 INFO streaming.PipeMapRed: R/W/S=200000/770954/0 in:40000=200000/5 [rec/s] out:154190=770954/5 [rec/s]\n",
      "16/01/31 19:59:59 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/01/31 20:00:00 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "16/01/31 20:00:02 INFO streaming.PipeMapRed: R/W/S=300000/1135849/0 in:37500=300000/8 [rec/s] out:141981=1135849/8 [rec/s]\n",
      "16/01/31 20:00:02 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/01/31 20:00:02 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 20:00:02 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 20:00:02 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/01/31 20:00:02 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 20:00:02 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 20:00:02 INFO mapred.MapTask: bufstart = 0; bufend = 12430561; bufvoid = 104857600\n",
      "16/01/31 20:00:02 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 21478268(85913072); length = 4736129/6553600\n",
      "16/01/31 20:00:03 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/31 20:00:04 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 20:00:04 INFO mapred.Task: Task:attempt_local1383108653_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 20:00:04 INFO mapred.LocalJobRunner: Records R/W=779/1\n",
      "16/01/31 20:00:04 INFO mapred.Task: Task 'attempt_local1383108653_0001_m_000000_0' done.\n",
      "16/01/31 20:00:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local1383108653_0001_m_000000_0\n",
      "16/01/31 20:00:04 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 20:00:04 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 20:00:04 INFO mapred.LocalJobRunner: Starting task: attempt_local1383108653_0001_r_000000_0\n",
      "16/01/31 20:00:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 20:00:04 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 20:00:04 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@448b5aa9\n",
      "16/01/31 20:00:04 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 20:00:04 INFO reduce.EventFetcher: attempt_local1383108653_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 20:00:04 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1383108653_0001_m_000000_0 decomp: 6472515 len: 6472519 to MEMORY\n",
      "16/01/31 20:00:04 INFO reduce.InMemoryMapOutput: Read 6472515 bytes from map-output for attempt_local1383108653_0001_m_000000_0\n",
      "16/01/31 20:00:04 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 6472515, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->6472515\n",
      "16/01/31 20:00:04 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 20:00:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 20:00:04 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 20:00:04 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 20:00:04 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 6472505 bytes\n",
      "16/01/31 20:00:04 INFO reduce.MergeManagerImpl: Merged 1 segments, 6472515 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 20:00:04 INFO reduce.MergeManagerImpl: Merging 1 files, 6472519 bytes from disk\n",
      "16/01/31 20:00:04 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 20:00:04 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 20:00:04 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 6472505 bytes\n",
      "16/01/31 20:00:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 20:00:04 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./wordcount_reducer.py]\n",
      "16/01/31 20:00:04 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 20:00:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 20:00:04 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:04 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:05 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 20:00:05 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:05 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:05 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:05 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:06 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 20:00:06 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 20:00:06 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 20:00:06 INFO streaming.PipeMapRed: Records R/W=518064/1\n",
      "16/01/31 20:00:06 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 20:00:06 INFO mapred.Task: Task:attempt_local1383108653_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 20:00:06 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 20:00:06 INFO mapred.Task: Task attempt_local1383108653_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 20:00:06 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1383108653_0001_r_000000_0' to hdfs://localhost:54310/user/dsq/issuecountoutput/_temporary/0/task_local1383108653_0001_r_000000\n",
      "16/01/31 20:00:06 INFO mapred.LocalJobRunner: Records R/W=518064/1 > reduce\n",
      "16/01/31 20:00:06 INFO mapred.Task: Task 'attempt_local1383108653_0001_r_000000_0' done.\n",
      "16/01/31 20:00:06 INFO mapred.LocalJobRunner: Finishing task: attempt_local1383108653_0001_r_000000_0\n",
      "16/01/31 20:00:06 INFO mapred.LocalJobRunner: Starting task: attempt_local1383108653_0001_r_000001_0\n",
      "16/01/31 20:00:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 20:00:06 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 20:00:06 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1ca408c6\n",
      "16/01/31 20:00:06 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 20:00:06 INFO reduce.EventFetcher: attempt_local1383108653_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 20:00:06 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1383108653_0001_m_000000_0 decomp: 8326116 len: 8326120 to MEMORY\n",
      "16/01/31 20:00:06 INFO reduce.InMemoryMapOutput: Read 8326116 bytes from map-output for attempt_local1383108653_0001_m_000000_0\n",
      "16/01/31 20:00:06 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 8326116, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8326116\n",
      "16/01/31 20:00:06 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 20:00:06 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 20:00:06 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 20:00:06 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 20:00:06 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8326107 bytes\n",
      "16/01/31 20:00:07 INFO reduce.MergeManagerImpl: Merged 1 segments, 8326116 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 20:00:07 INFO reduce.MergeManagerImpl: Merging 1 files, 8326120 bytes from disk\n",
      "16/01/31 20:00:07 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 20:00:07 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 20:00:07 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8326107 bytes\n",
      "16/01/31 20:00:07 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 20:00:07 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./wordcount_reducer.py]\n",
      "16/01/31 20:00:07 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/31 20:00:07 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 20:00:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:07 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:07 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:07 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:07 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:07 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:07 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:07 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:08 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 20:00:08 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 20:00:08 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 20:00:08 INFO streaming.PipeMapRed: Records R/W=665969/1\n",
      "16/01/31 20:00:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 20:00:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 20:00:08 INFO mapred.Task: Task:attempt_local1383108653_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/01/31 20:00:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 20:00:08 INFO mapred.Task: Task attempt_local1383108653_0001_r_000001_0 is allowed to commit now\n",
      "16/01/31 20:00:08 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1383108653_0001_r_000001_0' to hdfs://localhost:54310/user/dsq/issuecountoutput/_temporary/0/task_local1383108653_0001_r_000001\n",
      "16/01/31 20:00:08 INFO mapred.LocalJobRunner: Records R/W=665969/1 > reduce\n",
      "16/01/31 20:00:08 INFO mapred.Task: Task 'attempt_local1383108653_0001_r_000001_0' done.\n",
      "16/01/31 20:00:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local1383108653_0001_r_000001_0\n",
      "16/01/31 20:00:08 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 20:00:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 20:00:09 INFO mapreduce.Job: Job job_local1383108653_0001 completed successfully\n",
      "16/01/31 20:00:09 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=212466051\n",
      "\t\tFILE: Number of bytes written=237750966\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=152719458\n",
      "\t\tHDFS: Number of bytes written=2870\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1184033\n",
      "\t\tMap output bytes=12430561\n",
      "\t\tMap output materialized bytes=14798639\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=154\n",
      "\t\tReduce shuffle bytes=14798639\n",
      "\t\tReduce input records=1184033\n",
      "\t\tReduce output records=154\n",
      "\t\tSpilled Records=2368066\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=14\n",
      "\t\tTotal committed heap usage (bytes)=854065152\n",
      "\tHowMany\n",
      "\t\tReducer=2\n",
      "\tHowManyMappers\n",
      "\t\tMapper=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1985\n",
      "16/01/31 20:00:09 INFO streaming.StreamJob: Output directory: issuecountoutput\n"
     ]
    }
   ],
   "source": [
    "!echo\n",
    "!echo \"Running Hadoop Map Reducer for 3.2.a...\"\n",
    "!echo \"------------------------------------------\"\n",
    "!echo\n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.maps=2 \\\n",
    "-D mapreduce.job.reduces=2 \\\n",
    "-D mapreduce.job.name=\"Issue word count Mapper with Counters 3.2.a\" \\\n",
    "-mapper issuecount_mapper.py  -file issuecount_mapper.py \\\n",
    "-reducer wordcount_reducer.py -file wordcount_reducer.py \\\n",
    "-input Consumer_Complaints.csv -file Consumer_Complaints.csv \\\n",
    "-output issuecountoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Mappers and Reducers    \n",
    "\n",
    "    HowManyMappers\n",
    "\t\tMapper=1\n",
    "\tHowManyReducers\n",
    "\t\tReducer=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -ls issuecountoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat issuecountoutput/part-00000 | head -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (3.2.b) Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:***\n",
    "\n",
    "**For this question, Creating a separate Stand-alone Combiner and use with previously created mapper and reducer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing issuecount_combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile issuecount_combiner.py\n",
    "#!/usr/bin/python\n",
    "## issuecount_combiner.py\n",
    "##    - simple word count combiner\n",
    "##      for intermediate sorting.\n",
    "## Author: Tigi Thomas\n",
    "## Description: mapper code for HW 3.2.a\n",
    "\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HowMany,Combiner,1\\n\")\n",
    "    \n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning out old input and output files...\n",
      "------------------------------------------\n",
      "\n",
      "16/01/31 22:52:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 22:52:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/Consumer_Complaints.csv\n",
      "16/01/31 22:52:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/dsq/issuecountoutput/*': No such file or directory\n",
      "16/01/31 22:52:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 22:52:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Clear up prev inputs/outputs.\n",
    "!echo\n",
    "!echo \"Cleaning out old input and output files...\"\n",
    "!echo \"------------------------------------------\"\n",
    "!echo\n",
    "!hdfs dfs -rm /user/dsq/Consumer_Complaints.csv\n",
    "!hdfs dfs -rm /user/dsq/issuecountoutput/*\n",
    "!hdfs dfs -rmdir /user/dsq/issuecountoutput\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Hadoop Map Reducer for 3.2.b.. \n",
      "---------------------------------------\n",
      "\n",
      "16/01/31 22:53:50 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 22:53:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [issuecount_mapper.py, wordcount_reducer.py, wordcount_reducer.py, Consumer_Complaints.csv] [] /tmp/streamjob1191859889868039290.jar tmpDir=null\n",
      "16/01/31 22:53:53 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 22:53:53 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 22:53:53 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 22:53:54 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 22:53:54 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 22:53:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local275557709_0001\n",
      "16/01/31 22:53:55 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/issuecount_mapper.py as file:/app/hadoop/tmp/mapred/local/1454309634970/issuecount_mapper.py\n",
      "16/01/31 22:53:55 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/wordcount_reducer.py as file:/app/hadoop/tmp/mapred/local/1454309634971/wordcount_reducer.py\n",
      "16/01/31 22:53:55 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/Consumer_Complaints.csv as file:/app/hadoop/tmp/mapred/local/1454309634972/Consumer_Complaints.csv\n",
      "16/01/31 22:53:56 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 22:53:56 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 22:53:56 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 22:53:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 22:53:56 INFO mapreduce.Job: Running job: job_local275557709_0001\n",
      "16/01/31 22:53:56 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 22:53:56 INFO mapred.LocalJobRunner: Starting task: attempt_local275557709_0001_m_000000_0\n",
      "16/01/31 22:53:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 22:53:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 22:53:56 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/dsq/Consumer_Complaints.csv:0+50906486\n",
      "16/01/31 22:53:56 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/01/31 22:53:56 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 22:53:56 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 22:53:56 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 22:53:56 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 22:53:56 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 22:53:56 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 22:53:56 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./issuecount_mapper.py]\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 22:53:56 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 22:53:57 INFO mapreduce.Job: Job job_local275557709_0001 running in uber mode : false\n",
      "16/01/31 22:53:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 22:53:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:53:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:53:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:53:57 INFO streaming.PipeMapRed: Records R/W=779/1\n",
      "16/01/31 22:53:57 INFO streaming.PipeMapRed: R/W/S=1000/713/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:53:57 INFO streaming.PipeMapRed: R/W/S=10000/36117/0 in:10000=10000/1 [rec/s] out:36117=36117/1 [rec/s]\n",
      "16/01/31 22:54:00 INFO streaming.PipeMapRed: R/W/S=100000/388560/0 in:33333=100000/3 [rec/s] out:129520=388560/3 [rec/s]\n",
      "16/01/31 22:54:02 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/01/31 22:54:03 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/01/31 22:54:04 INFO streaming.PipeMapRed: R/W/S=200000/770184/0 in:28571=200000/7 [rec/s] out:110026=770184/7 [rec/s]\n",
      "16/01/31 22:54:05 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/01/31 22:54:06 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/01/31 22:54:07 INFO streaming.PipeMapRed: R/W/S=300000/1135849/0 in:30000=300000/10 [rec/s] out:113584=1135849/10 [rec/s]\n",
      "16/01/31 22:54:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 22:54:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 22:54:07 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/01/31 22:54:07 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 22:54:07 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 22:54:07 INFO mapred.MapTask: bufstart = 0; bufend = 12430577; bufvoid = 104857600\n",
      "16/01/31 22:54:07 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 21478264(85913056); length = 4736133/6553600\n",
      "16/01/31 22:54:08 INFO mapred.LocalJobRunner: Records R/W=779/1 > sort\n",
      "16/01/31 22:54:08 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./wordcount_reducer.py]\n",
      "16/01/31 22:54:08 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/01/31 22:54:08 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:08 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:08 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:08 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:08 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:09 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: Records R/W=518064/1\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./wordcount_reducer.py]\n",
      "16/01/31 22:54:09 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:09 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:10 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:10 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:10 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:10 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:10 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:10 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:10 INFO streaming.PipeMapRed: Records R/W=665970/1\n",
      "16/01/31 22:54:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 22:54:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 22:54:10 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 22:54:10 INFO mapred.Task: Task:attempt_local275557709_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 22:54:10 INFO mapred.LocalJobRunner: Records R/W=665970/1\n",
      "16/01/31 22:54:10 INFO mapred.Task: Task 'attempt_local275557709_0001_m_000000_0' done.\n",
      "16/01/31 22:54:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local275557709_0001_m_000000_0\n",
      "16/01/31 22:54:10 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 22:54:10 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 22:54:10 INFO mapred.LocalJobRunner: Starting task: attempt_local275557709_0001_r_000000_0\n",
      "16/01/31 22:54:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 22:54:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 22:54:10 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6064bec7\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 22:54:11 INFO reduce.EventFetcher: attempt_local275557709_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 22:54:11 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local275557709_0001_m_000000_0 decomp: 1023 len: 1027 to MEMORY\n",
      "16/01/31 22:54:11 INFO reduce.InMemoryMapOutput: Read 1023 bytes from map-output for attempt_local275557709_0001_m_000000_0\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1023, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1023\n",
      "16/01/31 22:54:11 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 22:54:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 22:54:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 22:54:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1013 bytes\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: Merged 1 segments, 1023 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: Merging 1 files, 1027 bytes from disk\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 22:54:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 22:54:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1013 bytes\n",
      "16/01/31 22:54:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 22:54:11 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./wordcount_reducer.py]\n",
      "16/01/31 22:54:11 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 22:54:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 22:54:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:11 INFO streaming.PipeMapRed: Records R/W=68/1\n",
      "16/01/31 22:54:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 22:54:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 22:54:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 22:54:11 INFO mapred.Task: Task:attempt_local275557709_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 22:54:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 22:54:11 INFO mapred.Task: Task attempt_local275557709_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 22:54:11 INFO output.FileOutputCommitter: Saved output of task 'attempt_local275557709_0001_r_000000_0' to hdfs://localhost:54310/user/dsq/issuecountoutput/_temporary/0/task_local275557709_0001_r_000000\n",
      "16/01/31 22:54:11 INFO mapred.LocalJobRunner: Records R/W=68/1 > reduce\n",
      "16/01/31 22:54:11 INFO mapred.Task: Task 'attempt_local275557709_0001_r_000000_0' done.\n",
      "16/01/31 22:54:11 INFO mapred.LocalJobRunner: Finishing task: attempt_local275557709_0001_r_000000_0\n",
      "16/01/31 22:54:11 INFO mapred.LocalJobRunner: Starting task: attempt_local275557709_0001_r_000001_0\n",
      "16/01/31 22:54:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 22:54:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/01/31 22:54:11 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@553c0dda\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 22:54:11 INFO reduce.EventFetcher: attempt_local275557709_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 22:54:11 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local275557709_0001_m_000000_0 decomp: 1292 len: 1296 to MEMORY\n",
      "16/01/31 22:54:11 INFO reduce.InMemoryMapOutput: Read 1292 bytes from map-output for attempt_local275557709_0001_m_000000_0\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1292, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1292\n",
      "16/01/31 22:54:11 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 22:54:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 22:54:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 22:54:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1282 bytes\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: Merged 1 segments, 1292 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: Merging 1 files, 1296 bytes from disk\n",
      "16/01/31 22:54:11 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 22:54:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 22:54:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1282 bytes\n",
      "16/01/31 22:54:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 22:54:11 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./wordcount_reducer.py]\n",
      "16/01/31 22:54:11 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 22:54:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 22:54:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 22:54:12 INFO streaming.PipeMapRed: Records R/W=87/1\n",
      "16/01/31 22:54:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 22:54:12 INFO mapred.Task: Task:attempt_local275557709_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/01/31 22:54:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 22:54:12 INFO mapred.Task: Task attempt_local275557709_0001_r_000001_0 is allowed to commit now\n",
      "16/01/31 22:54:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local275557709_0001_r_000001_0' to hdfs://localhost:54310/user/dsq/issuecountoutput/_temporary/0/task_local275557709_0001_r_000001\n",
      "16/01/31 22:54:12 INFO mapred.LocalJobRunner: Records R/W=87/1 > reduce\n",
      "16/01/31 22:54:12 INFO mapred.Task: Task 'attempt_local275557709_0001_r_000001_0' done.\n",
      "16/01/31 22:54:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local275557709_0001_r_000001_0\n",
      "16/01/31 22:54:12 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 22:54:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 22:54:12 INFO mapreduce.Job: Job job_local275557709_0001 completed successfully\n",
      "16/01/31 22:54:12 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=169925191\n",
      "\t\tFILE: Number of bytes written=172093862\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=152719458\n",
      "\t\tHDFS: Number of bytes written=2886\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1184034\n",
      "\t\tMap output bytes=12430577\n",
      "\t\tMap output materialized bytes=2323\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=1184034\n",
      "\t\tCombine output records=155\n",
      "\t\tReduce input groups=155\n",
      "\t\tReduce shuffle bytes=2323\n",
      "\t\tReduce input records=155\n",
      "\t\tReduce output records=155\n",
      "\t\tSpilled Records=310\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=463\n",
      "\t\tTotal committed heap usage (bytes)=1160773632\n",
      "\tHowMany\n",
      "\t\tReducer=4\n",
      "\tHowManyMappers\n",
      "\t\tMapper=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2001\n",
      "16/01/31 22:54:12 INFO streaming.StreamJob: Output directory: issuecountoutput\n"
     ]
    }
   ],
   "source": [
    "!echo\n",
    "!echo \"Running Hadoop Map Reducer for 3.2.b.. \"\n",
    "!echo \"---------------------------------------\"\n",
    "!echo\n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.maps=2 \\\n",
    "-D mapreduce.job.reduces=2 \\\n",
    "-D mapreduce.job.name=\"Testing How Many Mappers and Reducers again 3.2.b\" \\\n",
    "-mapper issuecount_mapper.py  -file issuecount_mapper.py \\\n",
    "-reducer wordcount_reducer.py -file wordcount_reducer.py \\\n",
    "-combiner wordcount_reducer.py -file wordcount_reducer.py \\\n",
    "-input Consumer_Complaints.csv -file Consumer_Complaints.csv \\\n",
    "-output issuecountoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Combiners, Mappers and Reducers    \n",
    "    HowManyCombiners\n",
    "\t\tCombiner=2\n",
    "\tHowManyMappers\n",
    "\t\tMapper=1\n",
    "\tHowManyReducers\n",
    "\t\tReducer=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2.c) Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting issuecount_mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile issuecount_mapper2.py\n",
    "#!/usr/bin/python\n",
    "## issuecount_mapper2.py\n",
    "##    - mapper for word & count from \n",
    "##      issue field in the Complaints dataset.csv\n",
    "## Author: Tigi Thomas\n",
    "## Description: mapper code for HW 3.2.a\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "#This will be our mapper count.\n",
    "sys.stderr.write(\"reporter:counter:HowMany,Mapper,1\\n\")\n",
    "\n",
    "#fixed list of stop words\n",
    "# ---------------------------------------\n",
    "#Not used...\n",
    "'''\n",
    "with open (stopwords, \"rU\") as stopwordf:\n",
    "     for line in stopwordf:\n",
    "        line = line.lower()\n",
    "        stopwords = line.split(\",\")\n",
    "'''\n",
    "# --------------------------------------\n",
    "\n",
    "#'''\n",
    "# Stop words to use \n",
    "stopwords = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "             'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','whya','will','with','would','yet','you','your']\n",
    "#'''\n",
    "\n",
    "#Use some pre-compiled re-gex for punctuation and numbers (exclude period and comma)\n",
    "punctpattern = re.compile(r'[\\.\\^\\$\\*\\+\\-\\=\\{\\}\\[\\]\\\\\\|\\(\\)<>-@&#%_=!?:;,/\\'\\\"]')\n",
    "#since we are going to consider period and comma as delimitter in addition to space\n",
    "nonprintable = re.compile('[^\\s!-~]')\n",
    "#take out numbers..\n",
    "numpatt1 = re.compile(r'\\b[0-9]{1,100}\\b')\n",
    "numpatt2 = re.compile(r'[0-9]{1,100}\\b')\n",
    "numpatt3 = re.compile(r'\\b[0-9]{1,100}')\n",
    "\n",
    "# Preprocess any text to remove punctuations, numbers, convert to lower etc.\n",
    "def preprocess_txt(s): \n",
    "    s = s.lower()\n",
    "    s = re.sub(nonprintable, r' ', s)\n",
    "    s = re.sub(punctpattern, r' ', s)\n",
    "    s = re.sub(numpatt1, r'', s )\n",
    "    s = re.sub(numpatt2, r'', s )\n",
    "    s = re.sub(numpatt3, r'', s )\n",
    "    return s\n",
    "\n",
    "# Print a set of counters...\n",
    "def printcounters(counters):\n",
    "    for counter, total in counters.iteritems():\n",
    "        sys.stderr.write(\"reporter:counter:ComplaintCategory,{0},{1}\\n\".format(counter,total))\n",
    "\n",
    "#Just keep tracks if we are done reading the header\n",
    "header_done = False \n",
    "total_words = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "#   since the data is comma separated, we use the csv.reader\n",
    "#   to get worry free comma delimitted values , even those\n",
    "#   inside quotes.\n",
    "for row in csv.reader(iter(sys.stdin.readline, '')):\n",
    "    if not header_done:\n",
    "        header_done = True;\n",
    "    else:\n",
    "        \n",
    "        # issue text is in the 4th field.\n",
    "        # remove leading and trailing whitespace\n",
    "        issuelog = row[3].strip()\n",
    "\n",
    "        # pre-process the line\n",
    "        issuelog = preprocess_txt(issuelog)\n",
    "\n",
    "        # split the line into words\n",
    "        words = issuelog.split()\n",
    "\n",
    "        #filter out the stop words\n",
    "        filtered_words = [tk for tk in words if tk not in stopwords]\n",
    "\n",
    "        # increase counters\n",
    "        for word in filtered_words:\n",
    "            # write the results to STDOUT \n",
    "            # tab-delimited; the trivial word count is 1\n",
    "            total_words += 1\n",
    "            print word + '\\t1'\n",
    "\n",
    "print '**total\\t{0}'.format(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting issuecount_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile issuecount_reducer.py\n",
    "#!/usr/bin/python\n",
    "## issuecount_reducer.py\n",
    "##     - This reducer is for counting word\n",
    "##       freq and relative word freq\n",
    "## Author: Tigi Thomas\n",
    "## Description: reducer code for HW 3.2.c \n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "#This custom counter , counts how many reducers.\n",
    "sys.stderr.write(\"reporter:counter:HowMany,Reducer,1\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "total_count = 0\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t')\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word and current_word != \"**total\":\n",
    "                print '{0}\\t{1}\\t{2:.5f}'.format(current_word, current_count, current_count/float(total_count))\n",
    "        elif (current_word == \"**total\"):\n",
    "                total_count = current_count    \n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "     print '{0}\\t{1}\\t{2:.5f}'.format(current_word, current_count, current_count/float(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:HowMany,Mapper,1\r\n",
      "reporter:counter:HowMany,Reducer,1\r\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "!cat Consumer_Complaints.csv | \\\n",
    "python issuecount_mapper2.py | \\\n",
    "LC_ALL=C sort -k1,1 | \\\n",
    "python issuecount_reducer.py > issuereduceroutput.txt\n",
    "\n",
    "#!cat Consumer_Complaints.csv | python issuecount_mapper.py | sort -k1,1 | python wordcount_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning out old input and output files...\n",
      "------------------------------------------\n",
      "\n",
      "16/02/01 00:15:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 00:15:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/Consumer_Complaints.csv\n",
      "16/02/01 00:15:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/dsq/issuecountoutput/*': No such file or directory\n",
      "16/02/01 00:15:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 00:15:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Clear up prev inputs/outputs.\n",
    "!echo\n",
    "!echo \"Cleaning out old input and output files...\"\n",
    "!echo \"------------------------------------------\"\n",
    "!echo\n",
    "!hdfs dfs -rm /user/dsq/Consumer_Complaints.csv\n",
    "!hdfs dfs -rm /user/dsq/issuecountoutput/*\n",
    "!hdfs dfs -rmdir /user/dsq/issuecountoutput\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display 50 Top\n",
    "\n",
    "**NOTE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Hadoop Map Reducer for 3.2.c.. \n",
      "---------------------------------------\n",
      "\n",
      "16/02/01 00:15:21 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 00:15:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [issuecount_mapper.py, issuecount_reducer.py, Consumer_Complaints.csv] [] /tmp/streamjob7798050576392304059.jar tmpDir=null\n",
      "16/02/01 00:15:24 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 00:15:24 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 00:15:24 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 00:15:26 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 00:15:26 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 00:15:26 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/01 00:15:26 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/01 00:15:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1849356516_0001\n",
      "16/02/01 00:15:27 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/issuecount_mapper.py as file:/app/hadoop/tmp/mapred/local/1454314526916/issuecount_mapper.py\n",
      "16/02/01 00:15:27 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/issuecount_reducer.py as file:/app/hadoop/tmp/mapred/local/1454314526917/issuecount_reducer.py\n",
      "16/02/01 00:15:27 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/Consumer_Complaints.csv as file:/app/hadoop/tmp/mapred/local/1454314526918/Consumer_Complaints.csv\n",
      "16/02/01 00:15:27 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 00:15:27 INFO mapreduce.Job: Running job: job_local1849356516_0001\n",
      "16/02/01 00:15:27 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 00:15:27 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 00:15:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 00:15:27 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 00:15:27 INFO mapred.LocalJobRunner: Starting task: attempt_local1849356516_0001_m_000000_0\n",
      "16/02/01 00:15:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 00:15:28 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/01 00:15:28 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/dsq/Consumer_Complaints.csv:0+50906486\n",
      "16/02/01 00:15:28 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 00:15:28 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 00:15:28 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 00:15:28 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 00:15:28 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 00:15:28 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 00:15:28 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 00:15:28 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./issuecount_mapper.py]\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 00:15:28 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 00:15:28 INFO mapreduce.Job: Job job_local1849356516_0001 running in uber mode : false\n",
      "16/02/01 00:15:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 00:15:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:29 INFO streaming.PipeMapRed: Records R/W=779/1\n",
      "16/02/01 00:15:29 INFO streaming.PipeMapRed: R/W/S=1000/533/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:29 INFO streaming.PipeMapRed: R/W/S=10000/37305/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:32 INFO streaming.PipeMapRed: R/W/S=100000/388560/0 in:33333=100000/3 [rec/s] out:129520=388560/3 [rec/s]\n",
      "16/02/01 00:15:34 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/02/01 00:15:34 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "16/02/01 00:15:35 INFO streaming.PipeMapRed: R/W/S=200000/770572/0 in:33333=200000/6 [rec/s] out:128428=770572/6 [rec/s]\n",
      "16/02/01 00:15:37 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/02/01 00:15:37 INFO streaming.PipeMapRed: R/W/S=300000/1135849/0 in:33333=300000/9 [rec/s] out:126205=1135849/9 [rec/s]\n",
      "16/02/01 00:15:37 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "16/02/01 00:15:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 00:15:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 00:15:38 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/02/01 00:15:38 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 00:15:38 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 00:15:38 INFO mapred.MapTask: bufstart = 0; bufend = 13614611; bufvoid = 104857600\n",
      "16/02/01 00:15:38 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 21478264(85913056); length = 4736133/6553600\n",
      "16/02/01 00:15:40 INFO mapred.LocalJobRunner: Records R/W=779/1 > sort\n",
      "16/02/01 00:15:41 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/01 00:15:41 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 00:15:41 INFO mapred.Task: Task:attempt_local1849356516_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 00:15:41 INFO mapred.LocalJobRunner: Records R/W=779/1\n",
      "16/02/01 00:15:41 INFO mapred.Task: Task 'attempt_local1849356516_0001_m_000000_0' done.\n",
      "16/02/01 00:15:41 INFO mapred.LocalJobRunner: Finishing task: attempt_local1849356516_0001_m_000000_0\n",
      "16/02/01 00:15:41 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 00:15:41 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 00:15:41 INFO mapred.LocalJobRunner: Starting task: attempt_local1849356516_0001_r_000000_0\n",
      "16/02/01 00:15:41 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 00:15:41 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/01 00:15:41 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@9f7cd4c\n",
      "16/02/01 00:15:41 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 00:15:41 INFO reduce.EventFetcher: attempt_local1849356516_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 00:15:41 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1849356516_0001_m_000000_0 decomp: 15982681 len: 15982685 to MEMORY\n",
      "16/02/01 00:15:41 INFO reduce.InMemoryMapOutput: Read 15982681 bytes from map-output for attempt_local1849356516_0001_m_000000_0\n",
      "16/02/01 00:15:41 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 15982681, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->15982681\n",
      "16/02/01 00:15:41 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 00:15:41 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 00:15:41 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 00:15:41 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 00:15:41 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 15982663 bytes\n",
      "16/02/01 00:15:42 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 00:15:42 INFO reduce.MergeManagerImpl: Merged 1 segments, 15982681 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 00:15:42 INFO reduce.MergeManagerImpl: Merging 1 files, 15982685 bytes from disk\n",
      "16/02/01 00:15:42 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 00:15:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 00:15:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 15982663 bytes\n",
      "16/02/01 00:15:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 00:15:42 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./issuecount_reducer.py]\n",
      "16/02/01 00:15:42 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 00:15:42 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 00:15:42 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:42 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:42 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:42 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:42 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:43 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:43 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:43 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:15:43 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 00:15:43 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 00:15:44 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 00:15:44 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:700000=700000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 00:15:44 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:800000=800000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 00:15:44 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 00:15:44 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 00:15:44 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:550000=1100000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 00:15:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 00:15:45 INFO streaming.PipeMapRed: Records R/W=1184034/1\n",
      "16/02/01 00:15:45 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 00:15:45 INFO mapred.Task: Task:attempt_local1849356516_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 00:15:45 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 00:15:45 INFO mapred.Task: Task attempt_local1849356516_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 00:15:45 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1849356516_0001_r_000000_0' to hdfs://localhost:54310/user/dsq/issuecountoutput/_temporary/0/task_local1849356516_0001_r_000000\n",
      "16/02/01 00:15:45 INFO mapred.LocalJobRunner: Records R/W=1184034/1 > reduce\n",
      "16/02/01 00:15:45 INFO mapred.Task: Task 'attempt_local1849356516_0001_r_000000_0' done.\n",
      "16/02/01 00:15:45 INFO mapred.LocalJobRunner: Finishing task: attempt_local1849356516_0001_r_000000_0\n",
      "16/02/01 00:15:45 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 00:15:46 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 00:15:46 INFO mapreduce.Job: Job job_local1849356516_0001 completed successfully\n",
      "16/02/01 00:15:46 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=145243102\n",
      "\t\tFILE: Number of bytes written=162674107\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=3217\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1184034\n",
      "\t\tMap output bytes=13614611\n",
      "\t\tMap output materialized bytes=15982685\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=155\n",
      "\t\tReduce shuffle bytes=15982685\n",
      "\t\tReduce input records=1184034\n",
      "\t\tReduce output records=154\n",
      "\t\tSpilled Records=2368068\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=613\n",
      "\t\tTotal committed heap usage (bytes)=836763648\n",
      "\tHowMany\n",
      "\t\tMapper=1\n",
      "\t\tReducer=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3217\n",
      "16/02/01 00:15:46 INFO streaming.StreamJob: Output directory: issuecountoutput\n"
     ]
    }
   ],
   "source": [
    "#-D stream.num.map.output.key.fields=2 \\\n",
    "#-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "#-D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "!echo\n",
    "!echo \"Running Hadoop Map Reducer for 3.2.c.. \"\n",
    "!echo \"---------------------------------------\"\n",
    "!echo\n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.maps=2 \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.name=\"Top 50 word count 3.2\" \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-mapper issuecount_mapper.py -file issuecount_mapper.py \\\n",
    "-reducer issuecount_reducer.py -file issuecount_reducer.py \\\n",
    "-input Consumer_Complaints.csv -file Consumer_Complaints.csv \\\n",
    "-output issuecountoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 00:39:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "account\t57448\t0.04852\n",
      "acct\t163\t0.00014\n",
      "action\t2964\t0.00250\n",
      "advance\t240\t0.00020\n",
      "advertising\t1193\t0.00101\n",
      "amount\t98\t0.00008\n",
      "amt\t71\t0.00006\n",
      "application\t8868\t0.00749\n",
      "applied\t139\t0.00012\n",
      "apply\t118\t0.00010\n"
     ]
    }
   ],
   "source": [
    "# Check the output from first mapper reducer\n",
    "!hdfs dfs -cat issuecountoutput/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write another mapper/reducer, just for sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sort_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sort_mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word, count, freq = line.strip().split('\\t')\n",
    "    print '{0}\\t{1}\\t{2}'.format(word, count, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sort_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sort_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word, count, freq = line.strip().split('\\t')\n",
    "    print '{0}\\t{1}\\t{2}'.format(word, count, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 00:51:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 00:51:14 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "account\t57448\t0.04852\n",
      "acct\t163\t0.00014\n",
      "action\t2964\t0.00250\n",
      "advance\t240\t0.00020\n",
      "advertising\t1193\t0.00101\n",
      "amount\t98\t0.00008\n",
      "amt\t71\t0.00006\n",
      "application\t8868\t0.00749\n",
      "applied\t139\t0.00012\n",
      "apply\t118\t0.00010\n"
     ]
    }
   ],
   "source": [
    "!rm HW32c_Issuecountoutput_Result.txt\n",
    "!hdfs dfs -copyToLocal issuecountoutput/part-00000 HW32c_Issuecountoutput_Result.txt\n",
    "!cat HW32c_Issuecountoutput_Result.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account\t57448\t0.04852\r\n",
      "acct\t163\t0.00014\r\n",
      "action\t2964\t0.00250\r\n",
      "advance\t240\t0.00020\r\n",
      "advertising\t1193\t0.00101\r\n",
      "amount\t98\t0.00008\r\n",
      "amt\t71\t0.00006\r\n",
      "application\t8868\t0.00749\r\n",
      "applied\t139\t0.00012\r\n",
      "apply\t118\t0.00010\r\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "!cat HW32c_Issuecountoutput_Result.txt | python sort_mapper.py | python sort_reducer.py > sortreducerout.txt\n",
    "!cat sortreducerout.txt| head -10\n",
    "#| \\\n",
    "#python issuecount_reducer.py > issuereduceroutput.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up previous run results.. \n",
      "---------------------------------------\n",
      "\n",
      "16/02/01 00:53:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 00:53:17 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/issuecountoutputsorted/_SUCCESS\n",
      "16/02/01 00:53:17 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/issuecountoutputsorted/part-00000\n",
      "16/02/01 00:53:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "Running Hadoop Map Reducer for 3.2.c.. \n",
      "---------------------------------------\n",
      "\n",
      "16/02/01 00:53:21 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 00:53:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [sort_mapper.py, sort_reducer.py] [] /tmp/streamjob1811918504640711650.jar tmpDir=null\n",
      "16/02/01 00:53:23 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 00:53:23 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 00:53:23 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 00:53:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 00:53:23 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 00:53:23 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/01 00:53:23 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/01 00:53:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local861284942_0001\n",
      "16/02/01 00:53:24 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/sort_mapper.py as file:/app/hadoop/tmp/mapred/local/1454316804087/sort_mapper.py\n",
      "16/02/01 00:53:24 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/sort_reducer.py as file:/app/hadoop/tmp/mapred/local/1454316804088/sort_reducer.py\n",
      "16/02/01 00:53:24 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 00:53:24 INFO mapreduce.Job: Running job: job_local861284942_0001\n",
      "16/02/01 00:53:24 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 00:53:24 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 00:53:24 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 00:53:24 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 00:53:24 INFO mapred.LocalJobRunner: Starting task: attempt_local861284942_0001_m_000000_0\n",
      "16/02/01 00:53:25 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 00:53:25 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/dsq/issuecountoutput/part-00000:0+3217\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./sort_mapper.py]\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: Records R/W=154/1\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 00:53:25 INFO mapred.LocalJobRunner: \n",
      "16/02/01 00:53:25 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: bufstart = 0; bufend = 3217; bufvoid = 104857600\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213784(104855136); length = 613/6553600\n",
      "16/02/01 00:53:25 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 00:53:25 INFO mapred.Task: Task:attempt_local861284942_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 00:53:25 INFO mapred.LocalJobRunner: Records R/W=154/1\n",
      "16/02/01 00:53:25 INFO mapred.Task: Task 'attempt_local861284942_0001_m_000000_0' done.\n",
      "16/02/01 00:53:25 INFO mapred.LocalJobRunner: Finishing task: attempt_local861284942_0001_m_000000_0\n",
      "16/02/01 00:53:25 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 00:53:25 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 00:53:25 INFO mapred.LocalJobRunner: Starting task: attempt_local861284942_0001_r_000000_0\n",
      "16/02/01 00:53:25 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 00:53:25 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "16/02/01 00:53:25 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@470d9d75\n",
      "16/02/01 00:53:25 INFO mapreduce.Job: Job job_local861284942_0001 running in uber mode : false\n",
      "16/02/01 00:53:25 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 00:53:25 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 00:53:25 INFO reduce.EventFetcher: attempt_local861284942_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 00:53:25 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local861284942_0001_m_000000_0 decomp: 3527 len: 3531 to MEMORY\n",
      "16/02/01 00:53:25 INFO reduce.InMemoryMapOutput: Read 3527 bytes from map-output for attempt_local861284942_0001_m_000000_0\n",
      "16/02/01 00:53:25 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 3527, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3527\n",
      "16/02/01 00:53:25 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 00:53:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 00:53:25 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 00:53:25 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 00:53:25 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3513 bytes\n",
      "16/02/01 00:53:25 INFO reduce.MergeManagerImpl: Merged 1 segments, 3527 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 00:53:25 INFO reduce.MergeManagerImpl: Merging 1 files, 3531 bytes from disk\n",
      "16/02/01 00:53:25 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 00:53:25 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 00:53:25 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3513 bytes\n",
      "16/02/01 00:53:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: PipeMapRed exec [/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week3_HadoopAlgos/HW3/./sort_reducer.py]\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 00:53:25 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:53:25 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 00:53:26 INFO streaming.PipeMapRed: Records R/W=154/1\n",
      "16/02/01 00:53:26 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 00:53:26 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 00:53:26 INFO mapred.Task: Task:attempt_local861284942_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 00:53:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 00:53:26 INFO mapred.Task: Task attempt_local861284942_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 00:53:26 INFO output.FileOutputCommitter: Saved output of task 'attempt_local861284942_0001_r_000000_0' to hdfs://localhost:54310/user/dsq/issuecountoutputsorted/_temporary/0/task_local861284942_0001_r_000000\n",
      "16/02/01 00:53:26 INFO mapred.LocalJobRunner: Records R/W=154/1 > reduce\n",
      "16/02/01 00:53:26 INFO mapred.Task: Task 'attempt_local861284942_0001_r_000000_0' done.\n",
      "16/02/01 00:53:26 INFO mapred.LocalJobRunner: Finishing task: attempt_local861284942_0001_r_000000_0\n",
      "16/02/01 00:53:26 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 00:53:26 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 00:53:26 INFO mapreduce.Job: Job job_local861284942_0001 completed successfully\n",
      "16/02/01 00:53:26 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9346\n",
      "\t\tFILE: Number of bytes written=571753\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6434\n",
      "\t\tHDFS: Number of bytes written=3217\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=154\n",
      "\t\tMap output records=154\n",
      "\t\tMap output bytes=3217\n",
      "\t\tMap output materialized bytes=3531\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=154\n",
      "\t\tReduce shuffle bytes=3531\n",
      "\t\tReduce input records=154\n",
      "\t\tReduce output records=154\n",
      "\t\tSpilled Records=308\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=23\n",
      "\t\tTotal committed heap usage (bytes)=362807296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3217\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3217\n",
      "16/02/01 00:53:26 INFO streaming.StreamJob: Output directory: issuecountoutputsorted\n"
     ]
    }
   ],
   "source": [
    "!echo\n",
    "!echo \"Cleaning up previous run results.. \"\n",
    "!echo \"---------------------------------------\"\n",
    "!echo\n",
    "!hdfs dfs -rm /user/dsq/issuecountoutputsorted/*\n",
    "!hdfs dfs -rmdir /user/dsq/issuecountoutputsorted\n",
    "\n",
    "!echo\n",
    "!echo \"Running Hadoop Map Reducer for 3.2.c.. \"\n",
    "!echo \"---------------------------------------\"\n",
    "!echo\n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.maps=2 \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.name=\"Top 50 word count\" \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-mapper sort_mapper.py -file sort_mapper.py \\\n",
    "-reducer sort_reducer.py -file sort_reducer.py \\\n",
    "-input issuecountoutput/* \\\n",
    "-output issuecountoutputsorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 00:56:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "loan\t119630\t0.10104\n",
      "collection\t72394\t0.06114\n",
      "foreclosure\t70487\t0.05953\n",
      "modification\t70487\t0.05953\n",
      "account\t57448\t0.04852\n",
      "credit\t55251\t0.04666\n",
      "payments\t39993\t0.03378\n",
      "escrow\t36767\t0.03105\n",
      "servicing\t36767\t0.03105\n",
      "report\t34903\t0.02948\n",
      "incorrect\t29133\t0.02460\n",
      "information\t29069\t0.02455\n",
      "debt\t27874\t0.02354\n",
      "closing\t19000\t0.01605\n",
      "attempts\t17972\t0.01518\n",
      "collect\t17972\t0.01518\n",
      "cont\t17972\t0.01518\n",
      "d\t17972\t0.01518\n",
      "owed\t17972\t0.01518\n",
      "management\t16205\t0.01369\n",
      "opening\t16205\t0.01369\n",
      "deposits\t10555\t0.00891\n",
      "withdrawals\t10555\t0.00891\n",
      "problems\t9484\t0.00801\n",
      "application\t8868\t0.00749\n",
      "communication\t8671\t0.00732\n",
      "tactics\t8671\t0.00732\n",
      "broker\t8625\t0.00728\n",
      "mortgage\t8625\t0.00728\n",
      "originator\t8625\t0.00728\n",
      "unable\t8178\t0.00691\n",
      "billing\t8158\t0.00689\n",
      "disclosure\t7655\t0.00647\n",
      "verification\t7655\t0.00647\n",
      "disputes\t6938\t0.00586\n",
      "reporting\t6559\t0.00554\n",
      "lease\t6337\t0.00535\n",
      "being\t5663\t0.00478\n",
      "caused\t5663\t0.00478\n",
      "funds\t5663\t0.00478\n",
      "low\t5663\t0.00478\n",
      "process\t5505\t0.00465\n",
      "managing\t5006\t0.00423\n",
      "improper\t4966\t0.00419\n",
      "company\t4858\t0.00410\n",
      "investigation\t4858\t0.00410\n",
      "s\t4858\t0.00410\n",
      "identity\t4729\t0.00399\n",
      "card\t4405\t0.00372\n",
      "score\t4357\t0.00368\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat issuecountoutputsorted/part-00000 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 00:56:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "apply\t118\t0.00010\n",
      "amount\t98\t0.00008\n",
      "credited\t92\t0.00008\n",
      "payment\t92\t0.00008\n",
      "checks\t75\t0.00006\n",
      "convenience\t75\t0.00006\n",
      "amt\t71\t0.00006\n",
      "day\t71\t0.00006\n",
      "disclosures\t64\t0.00005\n",
      "missing\t64\t0.00005\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat issuecountoutputsorted/part-00000 | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
