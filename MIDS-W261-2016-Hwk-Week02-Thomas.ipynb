{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Section 3\n",
    "## Homework, Week 2\n",
    "## Name: T.Thomas\n",
    "## Email: tgthomas@berkeley.edu\n",
    "## Submission Date:  1/26/2016 5:00AM PST\n",
    "\n",
    "### nbviewer link:\n",
    "http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/xqq6kzefz3unn1s/MIDS-W261-2016-Hwk-Week02-Thomas.ipynb\n",
    "\n",
    "### pdf link:\n",
    "https://www.dropbox.com/s/bnmgs2tqxrvrd4n/MIDS-W261-2016-Hwk-Week02-Thomas.T.pdf?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 ASSIGNMENTS using Hadoop Streaming and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a race condition in the context of parallel computation? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel Computing:**\n",
    "Parallel computing is a form of computation in which many calculations are carried out simultaneously. This computational paradigm is often applied to large computational problems that can be divided into smaller disconnected computational tasks wich are then executed concurrently (\"in parallel\"). Eventually the different individual computations are aggregated/combined to yield results or for additional computations.\n",
    "\n",
    "However, software written to perform concurrent compuational operations are complicated,due to additional challenges and software bugs arising from having to use shared data and resources. ***Race conditions*** are one such category of bugs that arise when access/modification of shared data and resources are not synchronized.\n",
    "\n",
    "As an example, consider a typical sequential computational flow as follows. The program clearly prints 3 as expected (folowing the sequential flow)\n",
    "```python \n",
    "    var X = 1 #Shared Variable X    \n",
    "    def func_A\n",
    "        X = X + 1    \n",
    "    def func_B\n",
    "        X = X + 1\n",
    "    ...\n",
    "    ...\n",
    "    func_A()\n",
    "    func_B()\n",
    "    print X    \n",
    "    #-------prints 3\n",
    "```    \n",
    "\n",
    "Now consider a parallel computational scenario where two threads could simultaneously attempt to increment the variable X\n",
    "\n",
    "```python \n",
    "    var X = 1 #Shared Variable X    \n",
    "    def Thread_A\n",
    "         X = X + 1\n",
    "    def Thread_B\n",
    "         X = X + 1\n",
    "    ...\n",
    "    ...\n",
    "    #Threads A & B are simultaneously called (parallel)\n",
    "    \n",
    "    print X  #The output is non-deterministic \n",
    "    #-------prints ? \n",
    "```    \n",
    "Some scenarion that can happen when the above code is executed.\n",
    "\n",
    "* Scenario 1: X = X + 1 \n",
    "    - A and B could both read the initial value of X and then overwrite each other \n",
    "    - Final result: ***X = X + 1 = 2*** \n",
    "\n",
    "* Scenario 2: X = X + 2\n",
    "    - A reads and writes, then B reads and writes\n",
    "    - Final result: ***X = X + 2 = 3***\n",
    "    \n",
    "The value of X above is non-deterministic and this is due to the Race Condition. In the above case, depending on the execution platform, the computer could order the sequence of instructions in different ways leading to different/unpredictable values for X. This is a Race Condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is MapReduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a very high level MapReduce can be viewed as a programming paradigm that 'codifies' a generic “recipe” for processing large datasets - broken down into two steps or stages. In the   \n",
    "\n",
    "* ***First stage (map)***, a user-specified computation/transformation is applied in parallel over all input records in a dataset, yielding an intermediate data set,that is then fed into a   \n",
    "\n",
    "* ***Second stage (fold)*** user-specified computation that may aggregate the output from the stage. \n",
    "\n",
    "These two stages are typically referred to as ***map*** and ***fold***, taking roots from the concept of *higher order functions* found in functional programming languages. \n",
    "\n",
    "This is depicated in the image below: https://www.dropbox.com/s/r8ri91snatyojry/Mapreduce_Basic.png?dl=0\n",
    "<img src=\"Mapreduce_Basic.png\" height=\"500\" width=\"700\" border=100>\n",
    "\n",
    "*Source: Lin, J., & Dyer, C. (2010). Data-intensive text processing with MapReduce. San Rafael, CA: Morgan & Claypool Publishers.* \n",
    "\n",
    "We can consider *map* as a concise way to represent the parallel transformation of a dataset (as defined by the function ***$f$***. Similarly *fold* is a concise way to represent an aggregation operation, as defined by the function ***$g$***. A programmer who then is trying to process large datasets, will define these two stages of *map & fold* computations usually within the context of an execution framework which coordinates the actual processing.  \n",
    "\n",
    "Put another way, MapReduce can refer to three distinct but related concepts.\n",
    "\n",
    "* First MapReduce can be viewed as a programming model discussed above. \n",
    "* Second, MapReduce can be an execution framework that orchestrates  execution of programs written in the map/fold stages. \n",
    "* Finally, MapReduce can be viewed as a stack of software implementations of the programming model and the execution framework (eg. Hadoop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it differ from Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce as discussed above can be a combination of Software implemenation and execution framework that 'codifies' all the software parts required to run the map and reduce/fold stage. Besides just the software functionlity for the Map and Reduce/fold stages, any form of parallel processing implemenation must take into consideration issues like, failure of hardware running the parallel computations (data/code redundancy), moving data efficiently across the connected computing nodes (distributed data storage, high data throughput), ability to scale horizontally as data volume increases etc. A number of Open source implementations have emerged over the past years that implement such fault tolerant, effecient execution frameworks, allowing the developers to just focus on building the map and reduce code. \n",
    "\n",
    "Hadoop is one such Open Source implemenation of the MapReduce programming paradigm, complete with a Java-based software implementation and an execution framework that simplifies the building of map and reduce/fold stages in addition to providing fault tolerance, data throughput and scalable processing power. It is part of the Apache project sponsored by the Apache Software Foundation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is based on the MapReduce Programming paradigm (See What is MapReduce ? above)\n",
    "\n",
    "Hadoop provides a Java-based programming framework that implements MapReduce progamming paradigm. Hadoop does all the work in the back-end required to perform the distributed orchestration among the different compute nodes that will perform the map and reduce tasks, along with distributed data storage with the Hadoop Distributed File System - **HDFS**. Hadoop is configurable , provides fault tolerance and the user just needs to provide code for teh mapper and reducer and Hadoop does the rest.\n",
    "\n",
    "As an example consider a simple word count problem that can be solved in a distrbuted fashion,  having a mapper program (outputs every word occuring in a text) and a reducer program (aggregates counts for each of the words). The text input, mapper code and reducer code are provided to the hadoop execution framework , which then executes the map and fold/reduce stages and writes the reduced output to the configured output directory. In this example, the user only needs to provide code for the mapper and reducer and the input and output parameters. Hadoop does all the work of splitting the input among multiple mappers and then aggregrating the results with multiple reducers. \n",
    "\n",
    "*Note:* Below example runs on a single node cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.txt\n",
    "betty bought some butter \n",
    "but the butter betty bought was bitter\n",
    "so betty bought some better butter to make the bitter batter better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop_test_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hadoop_test_mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop_test_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hadoop_test_reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Mapper & Reducer using unix commandline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batter\t1\r\n",
      "better\t2\r\n",
      "betty\t3\r\n",
      "bitter\t2\r\n",
      "bought\t3\r\n",
      "but\t1\r\n",
      "butter\t3\r\n",
      "make\t1\r\n",
      "so\t1\r\n",
      "some\t2\r\n",
      "the\t2\r\n",
      "to\t1\r\n",
      "was\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat wordcount.txt | python hadoop_test_mapper.py | sort -k1,1 | python hadoop_test_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start All Hadoop Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create folders for output, delete previous results & copy our WordCount.txt to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:19:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:19:01 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/wordcount.txt\n",
      "16/01/25 19:19:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:19:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/wordcountOutput/_SUCCESS\n",
      "16/01/25 19:19:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/wordcountOutput/part-00000\n",
      "16/01/25 19:19:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/wordcountOutput/part-00001\n",
      "16/01/25 19:19:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/wordcountOutput/part-00002\n",
      "16/01/25 19:19:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:19:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Folder already exists\n",
    "#!hdfs dfs -mkdir -p /user/dsq\n",
    "\n",
    "!hdfs dfs -rm /user/dsq/wordcount.txt\n",
    "!hdfs dfs -rm /user/dsq/wordcountOutput/*\n",
    "!hdfs dfs -rmdir /user/dsq/wordcountOutput\n",
    "!hdfs dfs -put wordcount.txt /user/dsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Run Hadoop Streaming job \n",
    "\n",
    "<pre>\n",
    "hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "    -D stream.num.map.output.key.fields=n \\\n",
    "    -mapper hadoop_test_mapper.py \\\n",
    "    -reducer hadoop_test_reducer.py \\\n",
    "    -input wordcount.txt \\\n",
    "    -output wordcountOutput</pre>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:20:05 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/25 19:20:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:20:06 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/25 19:20:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [hadoop_test_mapper.py, hadoop_test_reducer.py] [] /tmp/streamjob1500987317932814861.jar tmpDir=null\n",
      "16/01/25 19:20:06 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 19:20:06 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 19:20:06 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 19:20:07 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 19:20:07 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 19:20:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1302042517_0001\n",
      "16/01/25 19:20:08 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week2_MapReduce/hadoop_test_mapper.py as file:/app/hadoop/tmp/mapred/local/1453778408056/hadoop_test_mapper.py\n",
      "16/01/25 19:20:08 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week2_MapReduce/hadoop_test_reducer.py as file:/app/hadoop/tmp/mapred/local/1453778408057/hadoop_test_reducer.py\n",
      "......\tFile System Counters\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=23\n",
      "\t\tMap output bytes=178\n",
      "\t\tMap output materialized bytes=230\n",
      "\t\tInput split bytes=97\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=13\n",
      "\t\tReduce shuffle bytes=230\n",
      "\t\tReduce input records=23\n",
      "\t\tReduce output records=13\n",
      "\t\tSpilled Records=46\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=142\n",
      "\t\tTotal committed heap usage (bytes)=445644800\n",
      "......16/01/25 19:20:11 INFO streaming.StreamJob: Output directory: wordcountOutput\n"
     ]
    }
   ],
   "source": [
    "#!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \n",
    "    # -mapper hadoop_test_mapper.py \n",
    "    # -reducer hadoop_test_reducer.py \n",
    "    # -input wordcount.txt -output wordcountOutput \n",
    "    # -file hadoop_test_mapper.py \n",
    "    # -file hadoop_test_reducer.py \n",
    "    # -jobconf mapred.reduce.tasks=1 \n",
    "\n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar -mapper hadoop_test_mapper.py -reducer hadoop_test_reducer.py -input wordcount.txt -output wordcountOutput -file hadoop_test_mapper.py -file hadoop_test_reducer.py -jobconf mapred.reduce.tasks=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check results\n",
    "Check results in our output folder wordcountOutput in hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:20:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "batter\t1\n",
      "better\t2\n",
      "betty\t3\n",
      "bitter\t2\n",
      "bought\t3\n",
      "but\t1\n",
      "butter\t3\n",
      "make\t1\n",
      "so\t1\n",
      "some\t2\n",
      "the\t2\n",
      "to\t1\n",
      "was\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.1. Sort in Hadoop MapReduce\n",
    "\n",
    "Given as input: Records of the form <integer, “NA”>, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form <integer, “NA”> in decreasing order; \n",
    "\n",
    "* What happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "* Write code to generate N  random records of the form <integer, “NA”>. Let N = 10,000.\n",
    "* Write the python Hadoop streaming map-reduce job to perform this sort. \n",
    "* Display the top 10 biggest numbers. Display the 10 smallest numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sortinput.txt with 10,000 random <integer,\"NA\"> pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sortinputgen.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sortinputgen.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Tigi Thomas\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "import numpy as np\n",
    "\n",
    "sort_raw = []\n",
    "for i in range(1,10001):\n",
    "    sort_raw.append([i,\"NA\"])\n",
    "    \n",
    "X = np.array(sort_raw)\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X = X[shuffle]\n",
    "\n",
    "for i in xrange(X.shape[0]):\n",
    "    print X[i][0] + '\\t' + X[i][1]  # is your pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the sortinput.txt. This will be passed to hadoop map/reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python sortinputgen.py > sortinput.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop_sort_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hadoop_sort_mapper.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop_sort_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hadoop_sort_reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generete the sorted output just with the reducer\n",
    "!cat sortinput.txt | python hadoop_sort_mapper.py | sort -g -r | python hadoop_sort_reducer.py > sortedout.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\tNA\r\n",
      "9999\tNA\r\n",
      "9998\tNA\r\n",
      "9997\tNA\r\n",
      "9996\tNA\r\n",
      "9995\tNA\r\n",
      "9994\tNA\r\n",
      "9993\tNA\r\n",
      "9992\tNA\r\n",
      "9991\tNA\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "#see the top 10\n",
    "!cat sortedout.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -mkdir -p /user/dsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:21:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:21:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/sortinput.txt\n",
      "16/01/25 19:21:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:21:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/sortoutput/_SUCCESS\n",
      "16/01/25 19:21:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/sortoutput/part-00000\n",
      "16/01/25 19:21:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:21:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/dsq/sortinput.txt\n",
    "!hdfs dfs -rm /user/dsq/sortoutput/*\n",
    "!hdfs dfs -rmdir /user/dsq/sortoutput\n",
    "!hdfs dfs -put sortinput.txt /user/dsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:23:53 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/25 19:23:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:23:54 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/25 19:23:54 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [sortinput.txt, hadoop_sort_mapper.py, hadoop_sort_reducer.py] [] /tmp/streamjob4947641602306020312.jar tmpDir=null\n",
      "......16/01/25 19:23:56 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week2_MapReduce/hadoop_sort_mapper.py as file:/app/hadoop/tmp/mapred/local/1453778636083/hadoop_sort_mapper.py\n",
      "16/01/25 19:23:56 INFO mapred.LocalDistributedCacheManager: Localized file:/home/dsq/Dropbox/DataScienceBerkeley/Semester4/W261/Week2_MapReduce/hadoop_sort_reducer.py as file:/app/hadoop/tmp/mapred/local/1453778636084/hadoop_sort_reducer.py\n",
      "......\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=78894\n",
      "\t\tMap output materialized bytes=98900\n",
      "\t\tInput split bytes=97\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10000\n",
      "\t\tReduce shuffle bytes=98900\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=40\n",
      "\t\tTotal committed heap usage (bytes)=552599552\n",
      "......16/01/25 19:23:59 INFO streaming.StreamJob: Output directory: sortoutput\n"
     ]
    }
   ],
   "source": [
    "# Run the Hadoop Streaming Job\n",
    "#!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \n",
    "#    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \n",
    "#    -D mapred.text.key.comparator.options='-n -r' \n",
    "#    -mapper hadoop_sort_mapper.py \n",
    "#    -reducer hadoop_sort_reducer.py \n",
    "#    -input sortinput.txt \n",
    "#    -output sortoutput \n",
    "#    -file sortinput.txt \n",
    "#    -file hadoop_sort_mapper.py \n",
    "#    -file hadoop_sort_reducer.py \n",
    "#    -jobconf mapred.reduce.tasks=1\n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options='-n -r' -mapper hadoop_sort_mapper.py -reducer hadoop_sort_reducer.py -input sortinput.txt -output sortoutput -file sortinput.txt -file hadoop_sort_mapper.py -file hadoop_sort_reducer.py -jobconf mapred.reduce.tasks=1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Display the top 10 biggest numbers\n",
    "\n",
    "**NOTE**\n",
    "The reducer was configured to sort in reverse numerical key, so ***head -10*** will show the top 10 biggest numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:24:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "10000\tNA\n",
      "9999\tNA\n",
      "9998\tNA\n",
      "9997\tNA\n",
      "9996\tNA\n",
      "9995\tNA\n",
      "9994\tNA\n",
      "9993\tNA\n",
      "9992\tNA\n",
      "9991\tNA\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat sortoutput/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the 10 smallest numbers\n",
    "\n",
    "**NOTE**\n",
    "The reducer was configured to sort in reverse numerical key, so ***tail -10*** will show the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:24:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "10\tNA\n",
      "9\tNA\n",
      "8\tNA\n",
      "7\tNA\n",
      "6\tNA\n",
      "5\tNA\n",
      "4\tNA\n",
      "3\tNA\n",
      "2\tNA\n",
      "1\tNA\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat sortoutput/part-00000 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### What happens if you have multiple reducers? Do you need additional steps? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate data is sorted by key and passed onto the Reducers. If there are multiple reducers, then no ordering relationship is guaranteed for keys across the different reducers. Hence if we do have multiple reducers, additional steps might be required to combine the sorted results output from each reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.2.  WORDCOUNT\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "\n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "8    \n",
    "*NOTE*  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper\n",
    "\n",
    "**Note**: Filtering out some stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop_wordcount_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hadoop_wordcount_mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Tigi Thomas\n",
    "## Description: mapper code for HW1.2-1.5\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#fixed list of stop words\n",
    "stopwords = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "             'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','why','will','with','would','yet','you','your']\n",
    "\n",
    "#Use some pre-compiled re-gex for punctuation and numbers (exclude period and comma)\n",
    "punctpattern = re.compile(r'[\\.\\^\\$\\*\\+\\-\\=\\{\\}\\[\\]\\\\\\|\\(\\)<>-@&#%_=!?:;,/\\'\\\"]')\n",
    "nonprintable = re.compile('[^\\s!-~]')\n",
    "#since we are going to consider period and comma as delimitter in addition to space\n",
    "\n",
    "#take out numbers..\n",
    "numpatt1 = re.compile(r'\\b[0-9]{1,100}\\b')\n",
    "numpatt2 = re.compile(r'[0-9]{1,100}\\b')\n",
    "numpatt3 = re.compile(r'\\b[0-9]{1,100}')\n",
    "\n",
    "# Preprocess any text to remove punctuations, numbers, convert to lower etc.\n",
    "def preprocess_txt(s): \n",
    "    s = s.lower()\n",
    "    s = re.sub(nonprintable, r' ', s)\n",
    "    s = re.sub(punctpattern, r' ', s)\n",
    "    s = re.sub(numpatt1, r'', s )\n",
    "    s = re.sub(numpatt2, r'', s )\n",
    "    s = re.sub(numpatt3, r'', s )\n",
    "    return s\n",
    "\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # pre-process the line\n",
    "    line = preprocess_txt(line)\n",
    "    \n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    \n",
    "    #filter out the stop words\n",
    "    filtered_words = [tk for tk in words if tk not in stopwords]\n",
    "    \n",
    "    # increase counters\n",
    "    for word in filtered_words:\n",
    "        # write the results to STDOUT \n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print word + '\\t' + '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop_wordcount_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hadoop_wordcount_reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HowManyReducers,Reducer,1\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/dsq/enronemail_1h.txt': No such file or directory\n",
      "rm: `/user/dsq/onlywordcountoutput/_temporary': Is a directory\n",
      "rmdir: `/user/dsq/onlywordcountoutput': Directory is not empty\n"
     ]
    }
   ],
   "source": [
    "# Clear the old input data and output data\n",
    "!hdfs dfs -rm /user/dsq/enronemail_1h.txt\n",
    "!hdfs dfs -rm /user/dsq/onlywordcountoutput/*\n",
    "!hdfs dfs -rmdir /user/dsq/onlywordcountoutput\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dsq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:04:10 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/26 00:04:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 00:04:11 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "\t\t...\t\t...\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=19476\n",
      "\t\tMap output bytes=173009\n",
      "\t\tMap output materialized bytes=211967\n",
      "\t\tInput split bytes=101\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4955\n",
      "\t\tReduce shuffle bytes=211967\n",
      "\t\tReduce input records=19476\n",
      "\t\tReduce output records=4955\n",
      "\t\tSpilled Records=38952\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=52\n",
      "\t\tTotal committed heap usage (bytes)=479199232\n",
      "\t\t...\t\t...16/01/26 00:04:21 INFO streaming.StreamJob: Output directory: onlywordcountoutput\n"
     ]
    }
   ],
   "source": [
    "#!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \n",
    "    # -mapper hadoop_wordcount_mapper.py \n",
    "    # -reducer hadoop_wordcount_reducer.py \n",
    "    # -input enronemail_1h.txt \n",
    "    # -output onlywordcountoutput \n",
    "    # -file hadoop_wordcount_mapper.py \n",
    "    # -file hadoop_wordcount_reducer.py \n",
    "    # -jobconf mapred.reduce.tasks=1 \n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar -mapper hadoop_wordcount_mapper.py -reducer hadoop_wordcount_reducer.py -input enronemail_1h.txt -output onlywordcountoutput -file hadoop_wordcount_mapper.py -file hadoop_wordcount_reducer.py -jobconf mapred.reduce.tasks=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the word “assistance” and report its word count results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:06:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "assistance\t10\n"
     ]
    }
   ],
   "source": [
    "#Check Count of \"assistance\"\n",
    "!hdfs dfs -cat onlywordcountoutput/part-00000 | grep assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.2.1  Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)\n",
    "\n",
    "***NOTE***\n",
    "\n",
    "To accomplish this \n",
    "\n",
    "* we can take the output from H2.2 'onlywordcountoutput/part-00000 and pass it thru a mapper that just reverses (word, count) to (count, word). \n",
    "* Use the hadoop_sort_reducer.py from HW2.1 to get it sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hadoop_wordcount_reverse_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hadoop_wordcount_reverse_mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # split the line into words\n",
    "    word, count = line.split('\\t',1)\n",
    "    \n",
    "    # We just reverse it as Count and Word\n",
    "    # This will be fed to reducer which we can configure \n",
    "    # to sort numerically\n",
    "    print '%s\\t%s' % (count, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:26:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/dsq/reversewordcountoutput/*': No such file or directory\n",
      "16/01/26 00:26:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rmdir: `/user/dsq/reversewordcountoutput': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#Clear any existing outputs.. we leave the input from HW2.2 intact\n",
    "!hdfs dfs -rm /user/dsq/reversewordcountoutput/*\n",
    "!hdfs dfs -rmdir /user/dsq/reversewordcountoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:26:43 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/26 00:26:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\t...\t\t...\tMap-Reduce Framework\n",
      "\t\tMap input records=4955\n",
      "\t\tMap output records=4955\n",
      "\t\tMap output bytes=48863\n",
      "\t\tMap output materialized bytes=58779\n",
      "\t\tInput split bytes=114\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=67\n",
      "\t\tReduce shuffle bytes=58779\n",
      "\t\tReduce input records=4955\n",
      "\t\tReduce output records=4955\n",
      "\t\tSpilled Records=9910\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=101\n",
      "\t\tTotal committed heap usage (bytes)=477626368\n",
      "\t\t...\t\t...16/01/26 00:26:52 INFO streaming.StreamJob: Output directory: reversewordcountoutput\n"
     ]
    }
   ],
   "source": [
    "#!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \n",
    "    # -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \n",
    "    # -D mapred.text.key.comparator.options='-n -r' \n",
    "    # -mapper hadoop_wordcount_reverse_mapper.py \n",
    "    # -reducer hadoop_sort_reducer.py \n",
    "    # -input onlywordcountoutput/part-00000 \n",
    "    # -output reversewordcountoutput \n",
    "    # -file hadoop_wordcount_reverse_mapper.py \n",
    "    # -file hadoop_sort_reducer.py \n",
    "    # -jobconf mapred.reduce.tasks=1 \n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options='-n -r' -mapper hadoop_wordcount_reverse_mapper.py -reducer hadoop_sort_reducer.py -input onlywordcountoutput/part-00000 -output reversewordcountoutput -file hadoop_wordcount_reverse_mapper.py -file hadoop_sort_reducer.py -jobconf mapred.reduce.tasks=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 00:28:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "382\tect\n",
      "227\tcom\n",
      "206\thou\n",
      "149\ts\n",
      "127\tenron\n",
      "86\tfree\n",
      "84\tplease\n",
      "76\t~\n",
      "71\tinformation\n",
      "70\tout\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat reversewordcountoutput/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.3. Multinomial NAIVE BAYES with NO Smoothing \n",
    "\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. \n",
    "Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. \n",
    "\n",
    "**Count up how many times you need to process a zero probabilty for each class and report.**\n",
    "\n",
    "**Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the  posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see.**\n",
    "\n",
    "*Error Rate* = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper code for NB Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Tigi Thomas\n",
    "## Description: mapper code for HW2.3 - 2.5\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#fixed list of stop words\n",
    "stopwords = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "             'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','why','will','with','would','yet','you','your']\n",
    "\n",
    "#Use some pre-compiled re-gex for punctuation and numbers (exclude period and comma)\n",
    "punctpattern = re.compile(r'[\\.\\^\\$\\*\\+\\-\\=\\{\\}\\[\\]\\\\\\|\\(\\)<>-@&#%_=!?:;,/\\'\\\"]')\n",
    "nonprintable = re.compile('[^\\s!-~]')\n",
    "#since we are going to consider period and comma as delimitter in addition to space\n",
    "\n",
    "#take out numbers..\n",
    "numpatt1 = re.compile(r'\\b[0-9]{1,100}\\b')\n",
    "numpatt2 = re.compile(r'[0-9]{1,100}\\b')\n",
    "numpatt3 = re.compile(r'\\b[0-9]{1,100}')\n",
    "\n",
    "# Preprocess any text to remove punctuations, numbers, convert to lower etc.\n",
    "def preprocess_txt(s): \n",
    "    s = s.lower()\n",
    "    s = re.sub(nonprintable, r' ', s)\n",
    "    s = re.sub(punctpattern, r' ', s)\n",
    "    s = re.sub(numpatt1, r'', s )\n",
    "    s = re.sub(numpatt2, r'', s )\n",
    "    s = re.sub(numpatt3, r'', s )\n",
    "    return s\n",
    "\n",
    "#initialize arguments\n",
    "count = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "        # remove leading and trailing whitespace - some clean-up\n",
    "        line = line.strip().lower()\n",
    "        emaildata = line.split(\"\\t\")\n",
    "        emaildatalen = len(emaildata)\n",
    "       \n",
    "        #extrat the email parts.. \n",
    "        #don't really care about the first item, \n",
    "        #start from spam or not.. \n",
    "        if(emaildatalen >= 2):            \n",
    "            if(emaildatalen >= 3 ):\n",
    "                subject = emaildata[2]\n",
    "            else:\n",
    "                subject = \" \"\n",
    "            \n",
    "            if(emaildatalen == 4 ):\n",
    "                body = emaildata[3]\n",
    "            else:\n",
    "                body = \" \"\n",
    "            \n",
    "            #Get subject and body of email together\n",
    "            emailcontent = subject + \" \"  + body \n",
    "            \n",
    "            #pre-process to remove punctuation etc.\n",
    "            emailcontent = preprocess_txt(emailcontent)\n",
    "            \n",
    "            #extract words now and filter out stop-words\n",
    "            words = emailcontent.split()\n",
    "            filtered_words = [tk for tk in words if tk not in stopwords]\n",
    "\n",
    "            #we write the ID , current Spam classification, total word count in document\n",
    "            #the output file will contain one row for every document\n",
    "            if emaildata[1] == \"1\":\n",
    "                outtxt = emaildata[0]+'\\t'+ \"1\" +'\\t'+ str(len(filtered_words)) \n",
    "            else:\n",
    "                outtxt = emaildata[0]+'\\t'+ \"0\" +'\\t'+ str(len(filtered_words)) \n",
    "    \n",
    "            #Prepare our word count list\n",
    "            #Add word to dict if in findword list or all words '*' specified\n",
    "            wc = {}\n",
    "            for word in filtered_words:\n",
    "                    wc[word] = wc.get(word, 0) + 1\n",
    "            \n",
    "            #Stick the occurence of word=count for every word in document\n",
    "            #for faster processing on the reducer side.\n",
    "            wordcounttxt = \"\\t*\"\n",
    "            if(len(wc) > 0):\n",
    "                wordcounttxt = \"\";\n",
    "                for word, count in wc.iteritems():\n",
    "                    wordcounttxt += '\\t'+word+\"=\"+str(count)\n",
    "            \n",
    "            #write out document row with \n",
    "            print outtxt + wordcounttxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer Code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: T.Thomas\n",
    "## Description: reducer code for HW1.2-1.5\n",
    "import sys\n",
    "import math\n",
    "\n",
    "\n",
    "def printcollection(Title, coll):\n",
    "    if(Title != \"\"):\n",
    "        print \"\"\n",
    "        print Title          \n",
    "    for k, v in coll.iteritems():\n",
    "        print k, v\n",
    "\n",
    "\n",
    "# some dict structures to keep track of intermediate data\n",
    "wc = {}\n",
    "spwc = {}\n",
    "nb = {}\n",
    "w_condprob = {}\n",
    "testdata = {}\n",
    "\n",
    "#Change here to control smoothing...\n",
    "#lp_smooth = False  #--For HW 2.3\n",
    "lp_smooth = True #--For HW 2.4, 2.5\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    item = line.split(\"\\t\")\n",
    "\n",
    "    #extract total spam=1 and spam=0 doc/email counts\n",
    "    nb[\"spam\"+item[1]+\"dc\"] = nb.get(\"spam\"+item[1]+\"dc\", 0) + 1\n",
    "    #extract spam=1 and spam=0 word counts\n",
    "    nb[\"spam\"+item[1]+\"wc\"] = nb.get(\"spam\"+item[1]+\"wc\", 0) + eval(item[2])\n",
    "\n",
    "    #extract the words and their counts per document\n",
    "    #this way we have how many times word occurs in document\n",
    "    #and how many times it occurs in a spam document\n",
    "    if(item[3].strip() != \"*\"):\n",
    "        for itm in item[3:]:\n",
    "            word, count = itm.strip().split(\"=\",1)\n",
    "            wc[word] = wc.get(word, 0) + eval(count)\n",
    "            if(item[1] == \"1\"):                  \n",
    "                spwc[word] = spwc.get(word, 0) + eval(count)\n",
    "            #nb[item[0]+\"_InSpamCount\"] = nb.get(item[0]+\"_InSpamCount\", 0) + eval(item[2])\n",
    "    \n",
    "    #storing test data later for classification. \n",
    "    testdata[item[0]] = item[1:]\n",
    "\n",
    "#Calculate prior probabilities - these will be used everywhere.            \n",
    "prior_spam =  ( nb[\"spam1dc\"] / float( nb[\"spam1dc\"] + nb[\"spam0dc\"] ) ) \n",
    "prior_notspam = 1 - prior_spam \n",
    "\n",
    "#setup smoothing parameters if smoothing was specified. see line #58-59 in pNaiveBayes.sh\n",
    "lp_num = 0\n",
    "lp_denom = 0   \n",
    "if (lp_smooth):\n",
    "    lp_num = 1\n",
    "    lp_denom = len(wc) \n",
    "    \n",
    "#Here we pre-compute the conditional probablitly for each word\n",
    "#P(word in email | spam) &  #P(word in email | not spam)\n",
    "for word, count in wc.iteritems():\n",
    "    in_spam_count = spwc.get(word,0)\n",
    "    not_in_spam_count = (count - in_spam_count)\n",
    "    spam_wordcount = nb.get(\"spam1wc\")\n",
    "    not_spam_wordcount = nb.get(\"spam0wc\")\n",
    "    \n",
    "    spam_condprob =  (in_spam_count + lp_num)/ float(spam_wordcount + lp_num)\n",
    "    notspam_condprob = (not_in_spam_count + lp_num) / float(not_spam_wordcount + lp_num)\n",
    "    \n",
    "    # ----------- Debug check if we have any prob that are 0.0 -----------------\n",
    "    # if(spam_condprob == 0.0 or notspam_condprob == 0.0):\n",
    "    #   print word, count, in_spam_count,  not_in_spam_count   \n",
    "    \n",
    "    #Store the Conditional spam and notspam prob for every word.\n",
    "    w_condprob[word] = [spam_condprob, notspam_condprob]\n",
    "          \n",
    "#print all words and the count\n",
    "#printcollection(\"-----------Total Word Count-----------------\", wc)\n",
    "\n",
    "#print in-spam count for each word\n",
    "#printcollection(\"-----------In Spam Word Count-----------------\", spwc)\n",
    "\n",
    "#print some NB model parameters\n",
    "#printcollection(\"-----------Counts for Naive Bayes Model -------------\", nb)\n",
    "\n",
    "#print computed probabilities after running thru training set\n",
    "#print \"\"\n",
    "#print \"------Computed Probabilities from Training Set---\"\n",
    "#print \"P(prior_spam) = {0:.5f}\".format(prior_spam)\n",
    "#print \"P(prior_not_Spam) = {0:.5f}\".format(prior_notspam)\n",
    "\n",
    "#print \"\"\n",
    "#print \"Word [ P(spam | word in email), P(not spam | word in email) ]\"\n",
    "#print \"-------------------------------------------------------------\"\n",
    "#printcollection(\"\", w_condprob)\n",
    "\n",
    "\n",
    "#print classification - classify test data set\n",
    "#print \"\"\n",
    "#print \"RESULTS: Classification of Test Data ----------\"\n",
    "#print \"\"\n",
    "#print \"ID  Truth(Spam/Ham : 1/0) Class(Spam/Ham : 1/0)\"\n",
    "#print \"-----------------------------------------------\"\n",
    "result = {}\n",
    "\n",
    "#run the model on our data set classifying all emails.\n",
    "#note: \n",
    "#the first item in w_condprob, namely w_condprob[word][0] stores P(word in email | spam)\n",
    "#the second item in w_condprob, namely w_condprob[word][1] stores P(word in email | not spam)\n",
    "doc_count = len(testdata)\n",
    "error_count = 0\n",
    "zeroprobcount = 0\n",
    "for docid, data in testdata.iteritems():\n",
    "    #data[0] is current classification\n",
    "    #data[1] is total word count in the document\n",
    "    #data[2:] contains all the individual words and their counts\n",
    "    #         and for each word we compute P(spam | word in email ) and P(not spam | word in email)\n",
    "    #         \n",
    "    p_spam = math.log(prior_spam)\n",
    "    p_notspam = math.log(prior_notspam)\n",
    "    if(data[2].strip() != \"*\"):\n",
    "        zeroprobcount = 0\n",
    "        for itm in data[2:]:            \n",
    "            word, count = itm.strip().split(\"=\",1)\n",
    "            if(w_condprob[word][0] > 0.0 and w_condprob[word][1] > 0.0):\n",
    "                p_spam += math.log(w_condprob[word][0])*eval(count)\n",
    "                p_notspam += math.log(w_condprob[word][1])*eval(count)\n",
    "            else:\n",
    "                zeroprobcount += 1\n",
    "                \n",
    "    true_spam_or_ham = data[0]\n",
    "    classified_spam_or_ham = \"0\"\n",
    "    if(p_spam > p_notspam): \n",
    "        classified_spam_or_ham = \"1\"\n",
    "    \n",
    "    #Print our classification....\n",
    "    print docid + '\\t' + true_spam_or_ham + '\\t' + classified_spam_or_ham + '\\t' + str(zeroprobcount) + '\\t' + str(p_spam) +'\\t' + str(p_notspam) \n",
    "    \n",
    "    if(true_spam_or_ham != classified_spam_or_ham):\n",
    "        error_count += 1\n",
    "            \n",
    "#print \"\"\n",
    "#print \"Model Error Rate = {0:.5f}\".format(error_count/float(doc_count))\n",
    "\n",
    "#print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean any existing outputs and Run Streaming MapReduce Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 04:09:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/dsq/HW23output/*': No such file or directory\n",
      "16/01/26 04:09:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rmdir: `/user/dsq/HW23output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/dsq/HW23output/*\n",
    "!hdfs dfs -rmdir /user/dsq/HW23output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 04:09:46 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/26 04:09:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 04:09:46 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/26 04:09:46 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [mapper.py, reducer.py, enronemail_1h.txt] [] /tmp/streamjob1484723572566919927.jar tmpDir=null\n",
      "\t\t...\n",
      "\t\t...\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=110369\n",
      "\t\tMap output materialized bytes=110752\n",
      "\t\tInput split bytes=101\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=110752\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=101\n",
      "\t\tTotal committed heap usage (bytes)=479199232\n",
      "\t\t...\n",
      "\t\t...\n",
      "16/01/26 04:09:54 INFO streaming.StreamJob: Output directory: HW23output\n"
     ]
    }
   ],
   "source": [
    "#!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \n",
    "    # -mapper mapper.py \n",
    "    # -reducer reducer.py \n",
    "    # -input enronemail_1h.txt \n",
    "    # -output HW23output \n",
    "    # -file mapper.py \n",
    "    # -file reducer.py \n",
    "    # -file enronemail_1h.txt \n",
    "    # -jobconf mapred.reduce.tasks=1\n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output HW23output -file mapper.py -file reducer.py -file enronemail_1h.txt -jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check output\n",
    "\n",
    "Copy output to local directory for further analysis, graphing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 04:10:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 04:10:09 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm HW23_Classification_Result.txt\n",
    "!hdfs dfs -copyToLocal HW23output/part-00000 HW23_Classification_Result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-using Training Error Function provided in master solution from HW1\n",
    "\n",
    "#### Run Training Error for HW 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 2.3 Multinomial NB Results via Hadoop MapReduce Implementation NO LP Smoothing\n",
      "Misclassification Error Rate \n",
      "Training error: 0.1\n"
     ]
    }
   ],
   "source": [
    "#Training Error Function\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "def calculate_training_error(pred, true):\n",
    "    \"\"\"Calculates the training error given a vector \n",
    "    of predictions and a vector of true classes\"\"\"\n",
    "    \n",
    "    num_wrong=0\n",
    "    for i in zip(pred,true):\n",
    "        if i[0]!=i[1]: #If predicted value doesn't equal true value, increment our count\n",
    "            num_wrong+=1\n",
    "            \n",
    "    #Divide number of incorrect examples by total number of examples in the data\n",
    "    print \"Training error: \"+str(num_wrong/float(len(pred)))\n",
    "    \n",
    "#HW 1.3 Evaluation Code\n",
    "# To install pandas: at the SHELL prompt type conda install pandas \n",
    "import pandas as pd #Use pandas to quickly read results from our output file\n",
    "#conda install pandas \n",
    "def eval_2_3():\n",
    "    with open('HW23_Classification_Result.txt','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print \"HW 2.3 Multinomial NB Results via Hadoop MapReduce Implementation NO LP Smoothing\"\n",
    "    print \"Misclassification Error Rate \" \n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_2_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.4 Repeat HW 2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "***NOTE***\n",
    "\n",
    "* Using the same code from 2.3, just setting the LP Smoothing flag to **True** to turn on LP Smoothing.\n",
    "* Run MapReduce and capture output to new Folder for HW24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 02:30:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 02:30:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/HW24output/_SUCCESS\n",
      "16/01/26 02:30:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dsq/HW24output/part-00000\n",
      "16/01/26 02:30:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/dsq/HW24output/*\n",
    "!hdfs dfs -rmdir /user/dsq/HW24output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 02:30:18 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/26 02:30:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 02:30:19 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/26 02:30:19 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [mapper.py, reducer.py, enronemail_1h.txt] [] /tmp/streamjob1124503106935049572.jar tmpDir=null\n",
      "\t\t...\n",
      "\t\t...\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=110369\n",
      "\t\tMap output materialized bytes=110752\n",
      "\t\tInput split bytes=101\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=110752\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=61\n",
      "\t\tTotal committed heap usage (bytes)=479199232\n",
      "\t\t...\n",
      "\t\t...\n",
      "\t\t16/01/26 02:30:27 INFO streaming.StreamJob: Output directory: HW24output\n"
     ]
    }
   ],
   "source": [
    "# !$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \n",
    "    # -mapper mapper.py \n",
    "    # -reducer reducer.py \n",
    "    # -input enronemail_1h.txt \n",
    "    # -output HW24output \n",
    "    # -file mapper.py \n",
    "    # -file reducer.py \n",
    "    # -file enronemail_1h.txt \n",
    "    # -jobconf mapred.reduce.tasks=1\n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output HW24output -file mapper.py -file reducer.py -file enronemail_1h.txt -jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 02:30:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 02:30:37 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm HW24_Classification_With_LPSmoothing_Result.txt\n",
    "!hdfs dfs -copyToLocal HW24output/part-00000 HW24_Classification_With_LPSmoothing_Result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Training evaluation for HW 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 2.4 Multinomial NB Results via Hadoop MapReduce Implementation With LP Smoothing\n",
      "Misclassification Error Rate\n",
      "Training error: 0.0\n"
     ]
    }
   ],
   "source": [
    "#conda install pandas\n",
    "import pandas as pd \n",
    "\n",
    "def eval_2_4():\n",
    "    with open('HW24_Classification_With_LPSmoothing_Result.txt','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print \"HW 2.4 Multinomial NB Results via Hadoop MapReduce Implementation With LP Smoothing\"\n",
    "    print \"Misclassification Error Rate\" \n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_2_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "With LP Smoothing the Mis-classification error rate was 0% versus about 10% without smoothing. The difference mainly arises due to words that are missing in the training set that actually occur in the test data. Without any smoothing, we don't assign any probability mass to those words that are missing. Just because we did not see a word in the training set, it does not mean that that it never show up in the real test documents. By Appling smoothing we are providing some minimum probability for every word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.5. Repeat HW 2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we write a new mapper that ignores words with freq < 3.  Then run using previous reducer (with smoothing from HW 2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting highfreqwords_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile highfreqwords_mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Tigi Thomas\n",
    "## Description: mapper code for HW2.3 - 2.5\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#fixed list of stop words\n",
    "stopwords = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "             'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','why','will','with','would','yet','you','your']\n",
    "\n",
    "#Use some pre-compiled re-gex for punctuation and numbers (exclude period and comma)\n",
    "punctpattern = re.compile(r'[\\.\\^\\$\\*\\+\\-\\=\\{\\}\\[\\]\\\\\\|\\(\\)<>-@&#%_=!?:;,/\\'\\\"]')\n",
    "nonprintable = re.compile('[^\\s!-~]')\n",
    "#since we are going to consider period and comma as delimitter in addition to space\n",
    "\n",
    "#take out numbers..\n",
    "numpatt1 = re.compile(r'\\b[0-9]{1,100}\\b')\n",
    "numpatt2 = re.compile(r'[0-9]{1,100}\\b')\n",
    "numpatt3 = re.compile(r'\\b[0-9]{1,100}')\n",
    "\n",
    "# Preprocess any text to remove punctuations, numbers, convert to lower etc.\n",
    "def preprocess_txt(s): \n",
    "    s = s.lower()\n",
    "    s = re.sub(nonprintable, r' ', s)\n",
    "    s = re.sub(punctpattern, r' ', s)\n",
    "    s = re.sub(numpatt1, r'', s )\n",
    "    s = re.sub(numpatt2, r'', s )\n",
    "    s = re.sub(numpatt3, r'', s )\n",
    "    return s\n",
    "\n",
    "#initialize arguments\n",
    "count = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "        # remove leading and trailing whitespace - some clean-up\n",
    "        line = line.strip().lower()\n",
    "        emaildata = line.split(\"\\t\")\n",
    "        emaildatalen = len(emaildata)\n",
    "       \n",
    "        #extrat the email parts.. \n",
    "        #don't really care about the first item, \n",
    "        #start from spam or not.. \n",
    "        if(emaildatalen >= 2):            \n",
    "            if(emaildatalen >= 3 ):\n",
    "                subject = emaildata[2]\n",
    "            else:\n",
    "                subject = \" \"\n",
    "            \n",
    "            if(emaildatalen == 4 ):\n",
    "                body = emaildata[3]\n",
    "            else:\n",
    "                body = \" \"\n",
    "            \n",
    "            #Get subject and body of email together\n",
    "            emailcontent = subject + \" \"  + body \n",
    "            \n",
    "            #pre-process to remove punctuation etc.\n",
    "            emailcontent = preprocess_txt(emailcontent)\n",
    "            \n",
    "            #extract words now and filter out stop-words\n",
    "            words = emailcontent.split()\n",
    "            filtered_words = [tk for tk in words if tk not in stopwords]\n",
    "\n",
    "            #we write the ID , current Spam classification, total word count in document\n",
    "            #the output file will contain one row for every document\n",
    "            if emaildata[1] == \"1\":\n",
    "                outtxt = emaildata[0]+'\\t'+ \"1\" +'\\t'+ str(len(filtered_words)) \n",
    "            else:\n",
    "                outtxt = emaildata[0]+'\\t'+ \"0\" +'\\t'+ str(len(filtered_words)) \n",
    "    \n",
    "            #Prepare our word count list\n",
    "            #Add word to dict if in findword list or all words '*' specified\n",
    "            wc = {}\n",
    "            for word in filtered_words:\n",
    "                    wc[word] = wc.get(word, 0) + 1\n",
    "            \n",
    "            #Stick the occurence of word=count for every word in document\n",
    "            #for faster processing on the reducer side.\n",
    "            if(len(wc) > 0):\n",
    "                wordcounttxt = \"\";\n",
    "                for word, count in wc.iteritems():\n",
    "                    # *************************************************\n",
    "                    # We Ignore words with Freq Count < 3\n",
    "                    # *************************************************\n",
    "                    if count > 2: \n",
    "                        wordcounttxt += '\\t'+word+\"=\"+str(count)\n",
    "            \n",
    "            #Just capture the case where there are not valid words\n",
    "            if wordcounttxt == \"\":\n",
    "                wordcounttxt = \"\\t*\"\n",
    "                \n",
    "            #write out document row with \n",
    "            print outtxt + wordcounttxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 03:25:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/dsq/HW25output/*': No such file or directory\n",
      "16/01/26 03:25:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Clean up old data output\n",
    "!hdfs dfs -rm /user/dsq/HW25output/*\n",
    "!hdfs dfs -rmdir /user/dsq/HW25output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 03:25:28 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/26 03:25:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 03:25:29 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/26 03:25:29 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "packageJobJar: [mapper.py, reducer.py, enronemail_1h.txt] [] /tmp/streamjob8898892588560369024.jar tmpDir=null\n",
      "\t\t...\n",
      "\t\t...\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=14797\n",
      "\t\tMap output materialized bytes=15036\n",
      "\t\tInput split bytes=101\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=15036\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=56\n",
      "\t\tTotal committed heap usage (bytes)=479199232\n",
      "\t\t...\n",
      "\t\t...\n",
      "16/01/26 03:25:49 INFO streaming.StreamJob: Output directory: HW25output\n"
     ]
    }
   ],
   "source": [
    "# !$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar \n",
    "    # -mapper highfreqwords_mapper.py \n",
    "    # -reducer reducer.py \n",
    "    # -input enronemail_1h.txt \n",
    "    # -output HW24output \n",
    "    # -file mapper.py \n",
    "    # -file reducer.py \n",
    "    # -file enronemail_1h.txt \n",
    "    # -jobconf mapred.reduce.tasks=1\n",
    "\n",
    "!$HADOOP_INSTALL/bin/hadoop jar $HADOOP_INSTALL/hadoop-streaming-2.7.1.jar -mapper highfreqwords_mapper.py -reducer reducer.py -input enronemail_1h.txt -output HW25output -file mapper.py -file reducer.py -file enronemail_1h.txt -jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘HW25_Classification_With_LPSmoothing_And_HighFreq_Words.txt’: No such file or directory\n",
      "16/01/26 03:27:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 03:27:08 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!rm HW25_Classification_With_LPSmoothing_And_HighFreq_Words.txt\n",
    "!hdfs dfs -copyToLocal HW25output/part-00000 HW25_Classification_With_LPSmoothing_And_HighFreq_Words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Training Evaluation for 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 2.5 Multinomial NB Results via Hadoop MapReduce Implementation With LP Smoothing With word freq > 2\n",
      "Misclassification Error Rate\n",
      "Training error: 0.13\n"
     ]
    }
   ],
   "source": [
    "#conda install pandas \n",
    "import pandas as pd \n",
    "\n",
    "def eval_2_5():\n",
    "    with open('HW25_Classification_With_LPSmoothing_And_HighFreq_Words.txt','rb') as f:\n",
    "        mr_data=pd.read_csv(f, sep='\\t', header=None)\n",
    "    print \"HW 2.5 Multinomial NB Results via Hadoop MapReduce Implementation With LP Smoothing With word freq > 2\"\n",
    "    print \"Misclassification Error Rate\" \n",
    "    calculate_training_error(mr_data[1],mr_data[2])\n",
    "\n",
    "eval_2_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.6 Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "* Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "* Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "* Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn\n",
    "\n",
    "## HW 2.6.1 OPTIONAL (note this exercise is a stretch HW and optional)\n",
    "* Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error \n",
    "* Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain. \n",
    "\n",
    "***NOTE:***\n",
    "* The following code assumes \"enronemail_1h.txt\" is in the same folder as this notebook.\n",
    "* Results from HW2.4 are hard coded into the results table for conveinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rates: Benchmark Comparison between Scikit Learn and HW 2.4 \n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scikit MultinomialNB</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scikit BernoulliNB</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HW 2.4 MulinomilNB</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy  Error\n",
       "0  Scikit MultinomialNB      1.00   0.00\n",
       "1    Scikit BernoulliNB      0.77   0.23\n",
       "2    HW 2.4 MulinomilNB      1.00   0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "## \n",
    "## Author: Tigi Thomas\n",
    "## Description: Benchmark Classifier using Scikit Learn for HW 1.6\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "#fixed list of stop words\n",
    "stopwords = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "           'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','why','will','with','would','yet','you','your']\n",
    "\n",
    "#Use some pre-compiled re-gex for punctuation and numbers\n",
    "punctpattern = re.compile(r'[\\.\\^\\$\\*\\+\\-\\=\\{\\}\\[\\]\\\\\\|\\(\\)<>-@#%_=!:;,/\\'\\\"]')\n",
    "numpatt1 = re.compile(r'\\b[0-9]{2,100}\\b')\n",
    "numpatt2 = re.compile(r'[0-9]{2,100}\\b')\n",
    "numpatt3 = re.compile(r'\\b[0-9]{2,100}')\n",
    "stpwordspattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b')\n",
    "\n",
    "# Preprocess any text\n",
    "def preprocess_txt(s): \n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(punctpattern, r'',s)\n",
    "    s = re.sub(numpatt1, r'', s )\n",
    "    s = re.sub(numpatt2, r'', s )\n",
    "    s = re.sub(numpatt3, r'', s )\n",
    "    s = re.sub(stpwordspattern, r'',s)\n",
    "    return s\n",
    "\n",
    "email = []\n",
    "emailclass = []\n",
    "data = []\n",
    "\n",
    "# Read Data File and setup Prep Data for Scikit Learn Model\n",
    "filename = \"enronemail_1h.txt\"\n",
    "with open (filename, \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        line = line.lower()\n",
    "        emaildata = line.split(\"\\t\")\n",
    "        emaildatalen = len(emaildata)\n",
    "        \n",
    "        #don't really care about the first item, \n",
    "        #start from spam or not.. \n",
    "        if(emaildatalen >= 2):            \n",
    "            if(emaildatalen >= 3 ):\n",
    "                subject = emaildata[2]\n",
    "            else:\n",
    "                subject = \" \"\n",
    "            \n",
    "            if(emaildatalen == 4 ):\n",
    "                body = emaildata[3]\n",
    "            else:\n",
    "                body = \" \"\n",
    "                    \n",
    "            emailcontent = subject + \" \"  + body \n",
    "            \n",
    "            #pre-process to remove punctuation etc.\n",
    "            emailcontent = preprocess_txt(emailcontent).strip();\n",
    "            \n",
    "            data.append([emaildata[0], emailcontent])\n",
    "            email.append(emailcontent)\n",
    "            emailclass.append(emaildata[1])       \n",
    "                      \n",
    "\n",
    "# Shuffle and create Training and Test data Set- both are same for now.\n",
    "emails = np.array(data)\n",
    "X, Y = np.array(email), np.array(emailclass)\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "emails = emails[shuffle]\n",
    "\n",
    "#Both Test and Train are same data set as \n",
    "#per instructions in HW.\n",
    "train_data, train_labels  = X, Y\n",
    "test_data,  test_labels   = X, Y\n",
    "\n",
    "##checks\n",
    "#print emails.shape\n",
    "#print 'training label shape:', train_labels.shape\n",
    "#print 'test label shape:', test_labels.shape\n",
    "\n",
    "#Initialize Count Vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "#fit and transform\n",
    "fitX_train_counts = count_vect.fit_transform(train_data)\n",
    "fitX_test_counts  = count_vect.fit_transform(test_data)\n",
    "\n",
    "\n",
    "# Run thru the different Scikit Learn Models and Compare accuracy.\n",
    "\n",
    "print \"Error Rates: Benchmark Comparison between Scikit Learn and HW 2.4 \"\n",
    "print \"----------------------------------------------------------------\"\n",
    "results = []\n",
    "\n",
    "# ----------------------- MultinomialNB --------------------------\n",
    "mnb = MultinomialNB() #(alpha = a)\n",
    "mnb.fit(fitX_train_counts, train_labels)\n",
    "\n",
    "# Predict using our Model\n",
    "mnb_predicted = mnb.predict(fitX_test_counts)\n",
    "accuracy  = round(np.where(mnb_predicted == test_labels, 1, 0).sum() / float(len(test_data)),5)\n",
    "error = round(1 - accuracy,5)\n",
    "#print 'Error Rate : Scikit-Learn Multinomia NB = {0:.5f}'.format(error)\n",
    "results.append(['Scikit MultinomialNB', accuracy, error])\n",
    "\n",
    "# ----------------------- BernoulliNB --------------------------\n",
    "bnb = BernoulliNB() #(alpha = a)\n",
    "bnb.fit(fitX_train_counts, train_labels)\n",
    "\n",
    "# Predict using our Model\n",
    "bnb_predicted = bnb.predict(fitX_test_counts)\n",
    "accuracy = round(np.where(bnb_predicted == test_labels, 1, 0).sum() / float(len(test_data)),5)\n",
    "error = round(1 - accuracy,5)\n",
    "#print 'Error Rate : Sciki-Learn Bernoulli NB = {0:.5f}'.format(error)\n",
    "results.append(['Scikit BernoulliNB', accuracy, error])\n",
    "\n",
    "# ----------------------- HW1.5 --------------------------\n",
    "#Add in error rate from HW1.5 above\n",
    "#print 'Error Rate : HW1.5 MulinomilNB = {0:.5f}'.format(0.0)\n",
    "results.append(['HW 2.4 MulinomilNB', 1.0, 0.0])\n",
    "\n",
    "# Tabularize the Results\n",
    "results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Error\"])\n",
    "display(results)\n",
    "#HTML(results.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)\n",
    "\n",
    "   When using both Scikit MultinomialNB and MultinomialNB fro m HW1.5, the error rates were zero. In both cases the both the entire data set was used to train and to test the model. Additionally, Smoothing was used in HW1.5, and the default Scikit MultinomialNB 'alpha' (Additive Laplace smoothing parameter (0 for no smoothing)) is 1.0 , resulting in similar results.\n",
    "\n",
    "### Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn\n",
    "\n",
    "   BernoulliNB implements the naive Bayes assuming each feature to be a binary-valued (Bernoulli, boolean) variable. Therefore samples are required to be represented as binary-valued feature vectors. The decision rule for Bernoulli NB explicitly penalizes non-occurrence of a feature versus MultinomialNB which ignores a non-occurring feature. Since the feature vectors are now yes or no vectors versus count vectors, with larger documents there is more chance of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
